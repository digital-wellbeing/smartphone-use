[["index.html", "Analysis Report Preface", " Analysis Report Niklas Johannes 2021-02-03 Preface On this site, I document all steps, from data processing to final analyses, for the project [project title.] "],["setting-up-study1.html", "1 Setting up", " 1 Setting up In this section, I process and analyze the data for Study 1. First, I load all libraries that we need for the analysis. The pacman package just makes it easier to load packages. Note that the final version of this page doesnt evaluate the code chunk below to avoid installing packages on your machine. If youre fine with them being installed, set eval = TRUE. If you run the entire project with the renv private library, installing packages shouldve happened with the renv::restore call. if (!requireNamespace(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) library(pacman) # load packages p_load( tidyverse, lubridate, here, MBESS, ggridges, GGally, ggalt, cowplot, brms, ggbeeswarm, extrafont ) # set seed set.seed(42) # set theme theme_set(theme_cowplot()) Below custom functions that I use throughout the project. dens_with_points &lt;- function( data, variable ) { p &lt;- ggplot(data, aes_string(x = variable, y = 0)) + geom_density_ridges( jittered_points = TRUE, position = &quot;raincloud&quot;, fill = &quot;darkslateblue&quot;, point_color = &quot;darkslateblue&quot;, color = &quot;darkslateblue&quot;, alpha = 0.5 ) + theme_cowplot() + theme( axis.line=element_blank(), axis.text.y=element_blank(), axis.ticks=element_blank(), axis.title.y=element_blank(), axis.title.x=element_blank(), legend.position=&quot;none&quot;, panel.background=element_blank(), panel.border=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), plot.background=element_blank() ) return(p) } word2num &lt;- function(word){ # added to deal with NA if (is.na(word)){ return(NA) } else { wsplit &lt;- strsplit(tolower(word),&quot; &quot;)[[1]] one_digits &lt;- list(zero=0, one=1, two=2, three=3, four=4, five=5, six=6, seven=7, eight=8, nine=9) teens &lt;- list(eleven=11, twelve=12, thirteen=13, fourteen=14, fifteen=15, sixteen=16, seventeen=17, eighteen=18, nineteen=19) ten_digits &lt;- list(ten=10, twenty=20, thirty=30, forty=40, fifty=50, sixty=60, seventy=70, eighty=80, ninety=90) doubles &lt;- c(teens,ten_digits) out &lt;- 0 i &lt;- 1 while(i &lt;= length(wsplit)){ j &lt;- 1 if(i==1 &amp;&amp; wsplit[i]==&quot;hundred&quot;) temp &lt;- 100 else if(i==1 &amp;&amp; wsplit[i]==&quot;thousand&quot;) temp &lt;- 1000 else if(wsplit[i] %in% names(one_digits)) temp &lt;- as.numeric(one_digits[wsplit[i]]) else if(wsplit[i] %in% names(teens)) temp &lt;- as.numeric(teens[wsplit[i]]) else if(wsplit[i] %in% names(ten_digits)) temp &lt;- (as.numeric(ten_digits[wsplit[i]])) if(i &lt; length(wsplit) &amp;&amp; wsplit[i+1]==&quot;hundred&quot;){ if(i&gt;1 &amp;&amp; wsplit[i-1] %in% c(&quot;hundred&quot;,&quot;thousand&quot;)) out &lt;- out + 100*temp else out &lt;- 100*(out + temp) j &lt;- 2 } else if(i &lt; length(wsplit) &amp;&amp; wsplit[i+1]==&quot;thousand&quot;){ if(i&gt;1 &amp;&amp; wsplit[i-1] %in% c(&quot;hundred&quot;,&quot;thousand&quot;)) out &lt;- out + 1000*temp else out &lt;- 1000*(out + temp) j &lt;- 2 } else if(i &lt; length(wsplit) &amp;&amp; wsplit[i+1] %in% names(doubles)){ temp &lt;- temp*100 out &lt;- out + temp } else{ out &lt;- out + temp } i &lt;- i + j } return(out) } } describe &lt;- function( dat, variable, trait = FALSE ){ # if variable is not repeated-measures, take only one measure per participant if (trait == TRUE){ dat &lt;- dat %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() } # then get descriptives descriptives &lt;- dat %&gt;% filter(!is.na(UQ(sym(variable)))) %&gt;% # remove missing values summarise( across( !! variable, list( mean = mean, sd = sd, median = median, min = min, max = max, cilow = ~Rmisc::CI(.x)[[3]], # lower CI cihigh = ~Rmisc::CI(.x)[[1]] # upper CI ) ) ) descriptives &lt;- descriptives %&gt;% # only keep measure rename_all( ~ str_remove( ., paste0(variable, &quot;_&quot;) ) ) %&gt;% mutate( variable = variable, range = max - min ) %&gt;% relocate(variable) %&gt;% relocate( range, .after = max ) return(descriptives) } # raincloud plot function from https://github.com/RainCloudPlots/RainCloudPlots/blob/master/tutorial_R/R_rainclouds.R # Defining the geom_flat_violin function ---- # Note: the below code modifies the # existing github page by removing a parenthesis in line 50 &quot;%||%&quot; &lt;- function(a, b) { if (!is.null(a)) a else b } geom_flat_violin &lt;- function(mapping = NULL, data = NULL, stat = &quot;ydensity&quot;, position = &quot;dodge&quot;, trim = TRUE, scale = &quot;area&quot;, show.legend = NA, inherit.aes = TRUE, ...) { layer( data = data, mapping = mapping, stat = stat, geom = GeomFlatViolin, position = position, show.legend = show.legend, inherit.aes = inherit.aes, params = list( trim = trim, scale = scale, ... ) ) } #&#39; @rdname ggplot2-ggproto #&#39; @format NULL #&#39; @usage NULL #&#39; @export GeomFlatViolin &lt;- ggproto(&quot;GeomFlatViolin&quot;, Geom, setup_data = function(data, params) { data$width &lt;- data$width %||% params$width %||% (resolution(data$x, FALSE) * 0.9) # ymin, ymax, xmin, and xmax define the bounding rectangle for each group data %&gt;% group_by(group) %&gt;% mutate( ymin = min(y), ymax = max(y), xmin = x, xmax = x + width / 2 ) }, draw_group = function(data, panel_scales, coord) { # Find the points for the line to go all the way around data &lt;- transform(data, xminv = x, xmaxv = x + violinwidth * (xmax - x) ) # Make sure it&#39;s sorted properly to draw the outline newdata &lt;- rbind( plyr::arrange(transform(data, x = xminv), y), plyr::arrange(transform(data, x = xmaxv), -y) ) # Close the polygon: set first and last point the same # Needed for coord_polar and such newdata &lt;- rbind(newdata, newdata[1, ]) ggplot2:::ggname(&quot;geom_flat_violin&quot;, GeomPolygon$draw_panel(newdata, panel_scales, coord)) }, draw_key = draw_key_polygon, default_aes = aes( weight = 1, colour = &quot;grey20&quot;, fill = &quot;white&quot;, size = 0.5, alpha = NA, linetype = &quot;solid&quot; ), required_aes = c(&quot;x&quot;, &quot;y&quot;) ) single_cloud &lt;- function( raw_data, summary_data, variable, color, title, trait = FALSE ){ # take only one row per person if it&#39;s a trait variable if (trait == TRUE){ raw_data &lt;- raw_data %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() } # the plot p &lt;- ggplot( raw_data %&gt;% mutate(Density = 1), aes( x = Density, y = get(variable) ) ) + geom_flat_violin( # the &quot;cloud&quot; position = position_nudge(x = .2, y = 0), adjust = 2, color = NA, fill = color, alpha = 0.5 ) + geom_point( # the &quot;rain&quot; position = position_jitter(width = .15), size = 1, color = color, alpha = 0.5 ) + geom_point( # the mean from the summary stats data = summary_data %&gt;% filter(variable == !! variable) %&gt;% mutate(Density = 1), aes( x = Density + 0.175, y = mean ), color = color, size = 2.5 ) + geom_errorbar( # error bars data = summary_data %&gt;% filter(variable == !! variable) %&gt;% mutate(Density = 1), aes( x = Density + 0.175, y = mean, ymin = cilow, ymax = cihigh ), width = 0, size = 0.8, color = color ) + ylab(title) + theme( axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.ticks.x = element_blank(), axis.title.y = element_blank(), axis.line = element_blank() ) + guides( color = FALSE, fill = FALSE ) + coord_flip() return(p) } lm_function &lt;- function( data, mapping, ... ){ p &lt;- ggplot( data = data, mapping = mapping ) + geom_point( color = &quot;#56B4E9&quot;, alpha = 0.5 ) + geom_smooth( method=lm, fill=&quot;#0072B2&quot;, color=&quot;#0072B2&quot;, ...) p } dens_function &lt;- function( data, mapping, ... ){ p &lt;- ggplot( data = data, mapping = mapping ) + geom_density(fill = &quot;#009E73&quot;, color = NA, alpha = 0.5) } model_diagnostics &lt;- function( model ){ plot_grid( pp_check( model, type = &quot;dens_overlay&quot;, nsamples = 100 ), pp_check( model, type = &quot;loo_pit_qq&quot;, nsamples = 100 ), pp_check( model, type = &quot;loo_pit_overlay&quot;, nsamples = 100 ), pp_check( model, type = &quot;stat&quot;, stat = &quot;median&quot;, nsamples = 100 ), pp_check( model, type = &quot;stat&quot;, stat = &quot;mean&quot;, nsamples = 100 ), labels = c(&quot;Density overlay&quot;, &quot;LOO-PIT QQ&quot;, &quot;LOO-PIT Uniform&quot;, &quot;Predicted medians&quot;, &quot;Predicted means&quot;), ncol = 2, label_size = 8, hjust = 0, vjust = 0, label_x = 0, label_y = 0.93 ) } "],["data-processing-study1.html", "2 Data processing Study 1 2.1 Personality data 2.2 Diary data 2.3 Screen time data 2.4 Merge data sets", " 2 Data processing Study 1 Lets process the raw data and get them to a format that we can work with for the analysis. Study 1 has three raw data files: personality_raw.csv is the survey participants filled out at the beginning of the study, reporting personality traits diary_raw.csv is the end of day survey participants filled out during the diary part of the study screen_time_raw.csv is the objective screen time data participants reported 2.1 Personality data Lets load the personality data. personality_raw &lt;- read_csv(here(&quot;data&quot;, &quot;study1&quot;, &quot;personality_raw.csv&quot;)) Qualtrics names arent very informative, so I give some sensible variable names (although this data set has mostly meaningful variable names). Also, there are variables (i.e., anything starting with afi to selfreflect) that were part of a different study, which is why I dont include them here. Note that I dont want to change the raw data object, so Ill keep using personality as a new object from here on. personality &lt;- personality_raw %&gt;% select( # meta data start_date = StartDate, end_date = EndDate, progress = Progress, duration = Duration__in_seconds_, finished = Finished, recorded_date = RecordedDate, id = study_id, date, # basic psychological need satisfaction trait bpns_ = starts_with(&quot;bpns&quot;), # big five big_five_ = starts_with(&quot;bfi&quot;), # demographics gender, gender_entry = gender_3_TEXT, age = Q42, ethnicity_american_indian = ethnicity_1, ethnicity_asian = ethnicity_2, ethnicity_island = ethnicity_3, ethnicity_black = ethnicity_4, ethnicity_white = ethnicity_5, ethnicity_hispanic = ethnicity_6, ethnicity_multi = ethnicity_7 ) Next, I make sure those variables have the correct variable type. Ill also recode factor levels so that labels are readable. Note that participants filled in their age from a dropdown menu to prevent typos. The dropdown went from age 18 (1) to 30+ (13). Nobody was older than thirty, so Ill recode age as well. personality &lt;- personality %&gt;% mutate( id = paste0(&quot;pp_&quot;, id), # add pp for participant just so later I don&#39;t mistake the id variable for a count across( # turn those two into factors c(finished, id, gender), as.factor ), finished = fct_recode( finished, &quot;yes&quot; = &quot;1&quot;, &quot;no&quot; = &quot;0&quot; ), gender = fct_recode( gender, &quot;male&quot; = &quot;1&quot;, &quot;female&quot; = &quot;2&quot;, &quot;other&quot; = &quot;3&quot; ), age = age + 17 ) I see that ethnicity was multiple choice although multiracial is an answer option. So I think it makes more sense to combine this all into one variable. That required quite a lot of code, but was still less problematic than transforming the ethnicity variables to long format. I took the following steps: check who reported multiple ethnicities those who did get an NA on all other ethnicity variables recoded the variables from numeric to containing the ethnicity factor level and coalesce personality &lt;- personality %&gt;% mutate( multiple = rowSums(select(., starts_with(&quot;ethnicity&quot;)), na.rm = TRUE), # check who reported multiple ethnicities # those with multiple get NA for all other ethnicity variables across( c(ethnicity_american_indian:ethnicity_hispanic), ~ case_when( multiple &gt; 1 ~ NA_real_, TRUE ~ . ) ), # turn into character which makes recoding below easier across( c(ethnicity_american_indian:ethnicity_hispanic), as.character ), # recode the individual variables ethnicity_american_indian = if_else(ethnicity_american_indian == 1, &quot;American Indian/Alaska Native&quot;, NA_character_), ethnicity_asian = if_else(ethnicity_asian == 1, &quot;Asian&quot;, NA_character_), ethnicity_island = if_else(ethnicity_island == 1, &quot;Native Hawaiian or Other Pacific Islander&quot;, NA_character_), ethnicity_black = if_else(ethnicity_black == 1, &quot;Black or African American&quot;, NA_character_), ethnicity_white = if_else(ethnicity_white == 1, &quot;White&quot;, NA_character_), ethnicity_hispanic = if_else(ethnicity_hispanic == 1, &quot;Hispanic or Latino&quot;, NA_character_), ethnicity_multi = if_else(multiple &gt; 1, &quot;Multiracial&quot;, NA_character_), # coalesce them (coalesce doesn&#39;t work with .select helpers, so needed to type the variables out) ethnicity = as.factor( coalesce( ethnicity_american_indian, ethnicity_asian, ethnicity_island, ethnicity_black, ethnicity_white, ethnicity_hispanic, ethnicity_multi ) ) ) %&gt;% select(start_date:age, ethnicity) Then I remove those cases without an id (aka test runs) and those with an id but not other entries. The latter are probably no-shows. Note: In the earlier versions of this script, I excluded per data set, which made it hard for reporting exact exclusions in the final paper. For example, at this point I dont know whether someone I exclude here even participated in the diary part of the study. Therefore, I save each exclusion criterion in a separate object and inspect how many people we actually need to exclude later in the data merging section. # remove test run personality &lt;- personality %&gt;% filter(id != &quot;pp_NA&quot;) personality_exclusions_1 &lt;- personality %&gt;% filter(is.na(start_date)) %&gt;% pull(id) Lets see how many missing values there are left per participant (aka row) in 2.1. Most participants filled out everything, but a couple of them have basically empty rows (especially those above with all empty rows). A couple of participants have quite a handful of empty cells. I inspected their data and most of them just didnt fill out the Big Five, but some participants didnt fill out anything. Table 2.1: Count of number of missing values per row na_count_row n 0 1 1 246 2 29 3 5 4 2 7 1 9 2 25 1 49 4 74 1 81 10 Ill exclude the latter. # exclude those with &gt; 15 missing values (which identifies empty rows) personality_exclusions_2 &lt;- personality %&gt;% mutate( na_count_row = rowSums(is.na(.)) ) %&gt;% filter(na_count_row &gt;70) %&gt;% pull(id) Alright, time to construct the means for the scales. But first, I check data quality by seeing whether there are instances of straightlining (i.e., choosing the same response option for all items of a scale). We also had attention checks in the survey. Lets have a look at those attention checks. For item bpns_25 participants had to choose 2 as a response and for item big_five_45 they had to choose 5. When we check Table 2.2, we see that 5 participants didnt pass both attention checks. Table 2.2: Failed attention checks bpns_25 != 2 n FALSE 277 TRUE 13 NA 12 big_five_45 != 5 n FALSE 275 TRUE 7 NA 20 (bpns_25 != 2 &amp; big_five_45 != 5) n FALSE 286 TRUE 5 NA 11 Ill exclude those participants because I dont think we can trust their data. personality_exclusions_3 &lt;- personality %&gt;% filter(bpns_25 != 2 &amp; big_five_45 != 5) %&gt;% pull(id) Next, I check for straightlining by inspecting the variance per daily survey on the BPNS and BF scales. Turns out that its not super straightforward to compute variance per row, found help here. personality &lt;- personality %&gt;% rowwise() %&gt;% mutate( bpns_sd = sd( c_across(starts_with(&quot;bpns&quot;)), na.rm = TRUE ), big_five_sd = sd( c_across(starts_with(&quot;big_five&quot;)), na.rm = TRUE ) ) %&gt;% unnest(cols = c()) If we visualize that variance in Figure 2.1, we can see that theres little cause for concern: few people straightlined. Figure 2.1: Distribution of variances per survey on two measures Those two are already captured by the exclusion criteria above (I checked manually, but Ill still save them in an object). personality_exclusions_4 &lt;- personality %&gt;% filter(bpns_sd == 0 | big_five_sd == 0) %&gt;% pull(id) Alright, now we can aggregate the individual items to scale scores. Some of those items are reverse-coded (see the codebook for details). Ill code all scales such that a higher scores means scoring more positively on that scale. personality &lt;- personality %&gt;% mutate( across( c( bpns_5:bpns_8, bpns_13:bpns_15, bpns_21:bpns_25, ), function(.) 8 - . ), across( c( big_five_2, big_five_6, big_five_8, big_five_9, big_five_12, big_five_18, big_five_21, big_five_23, big_five_27, big_five_31, big_five_37, big_five_43 ), function(.) 6 - . ) ) Now I create the scales by taking the mean of the respective items. Note that I remove missing values in computing row means to ignore the occasional missing value. (For those with missing values throughout, see next code chunk.) personality &lt;- personality %&gt;% mutate( # trait needs autonomy_trait = rowMeans(select(., bpns_1:bpns_8), na.rm = TRUE), competence_trait = rowMeans(select(., bpns_9:bpns_16), na.rm = TRUE), relatedness_trait = rowMeans(select(., bpns_17:bpns_24), na.rm = TRUE), # big five extraversion = rowMeans( select( ., big_five_1, big_five_6, big_five_11, big_five_16, big_five_21, big_five_26, big_five_31, big_five_36 ), na.rm = TRUE ), agreeableness = rowMeans( select( ., big_five_2, big_five_7, big_five_12, big_five_17, big_five_22, big_five_27, big_five_32, big_five_37, big_five_42 ), na.rm = TRUE ), conscientiousness = rowMeans( select( ., big_five_3, big_five_8, big_five_13, big_five_18, big_five_23, big_five_28, big_five_33, big_five_38, big_five_43 ), na.rm = TRUE ), neuroticism = rowMeans( select( ., big_five_4, big_five_9, big_five_14, big_five_19, big_five_24, big_five_29, big_five_34, big_five_39 ), na.rm = TRUE ), openness = rowMeans( select( ., big_five_5, big_five_10, big_five_15, big_five_20, big_five_25, big_five_30, big_five_35, big_five_40, big_five_41, big_five_44 ), na.rm = TRUE ) ) Last, some participants didnt fill out the Big Five items, so they haven an NaN entry for those scales now. Ill turn that into an NA value personality &lt;- personality %&gt;% mutate( across( c(autonomy_trait:openness), ~ na_if(.x, &quot;NaN&quot;) ) ) 2.2 Diary data Lets load and inspect the diary data. # load data diary_raw &lt;- read_csv(here(&quot;data&quot;, &quot;study1&quot;, &quot;diary_raw.csv&quot;)) Qualtrics names arent very informative, so I give some sensible variable names. Again, there are many variables (i.e., anything before Q21-1) that were part of a different study, which is why I dont include them here. Like before, I dont want to change the raw data object, so Ill keep using diary as a new object from here on. diary &lt;- diary_raw %&gt;% select( # meta data start_date = StartDate, end_date = EndDate, progress = Progress, duration = Duration__in_seconds_, finished = Finished, recorded_date = RecordedDate, id = study_id, day = Day, date = Q27, # social media subjective estimates hours_subjective = Q21_1, minutes_subjective = Q21_2, pickups_subjective = Q22_1, notifications_subjective = Q23_1, # arousal low_positive_peaceful = Q24_1, low_positive_calm = Q24_2, low_positive_related = Q24_3, high_negative_anxious = Q24_4, high_negative_jittery = Q24_5, high_negative_tense = Q24_6, high_positive_happy = Q24_7, high_positive_energized = Q24_8, high_positive_excited = Q24_9, low_negative_sluggish = Q24_10, low_negative_sad = Q24_11, low_negative_gloomy = Q24_12, # autonomy need satisfaction autonomy_ = Q25_1:Q25_4, # competence need satisfaction competence_ = Q25_5:Q25_8, # relatedness need satisfaction relatedness_ = Q25_9:Q25_12, # experiences satisfied = Q30_1, boring = Q30_2, stressful = Q30_3, enjoyable = Q30_4 ) Next, I make sure those variables have the correct variable type. Ill also recode factor levels so that labels are readable. Note that day goes from 1 to 5. All participants started on a Monday, which is why I turn day into a factor and reorder the levels. diary &lt;- diary %&gt;% mutate( id = paste0(&quot;pp_&quot;, id), # add pp for participant just so later I don&#39;t mistake the id variable for a count across( # turn those two into factors c(finished, id, day), as.factor ), finished = fct_recode( finished, &quot;yes&quot; = &quot;1&quot;, &quot;no&quot; = &quot;0&quot; ), day = fct_recode( day, &quot;monday&quot; = &quot;1&quot;, &quot;tuesday&quot; = &quot;2&quot;, &quot;wednesday&quot; = &quot;3&quot;, &quot;thursday&quot; = &quot;4&quot;, &quot;friday&quot; = &quot;5&quot; ) ) Then I identify all empty rows (i.e., rows without a start date, so people who have placeholder id, but didnt participate on that day). diary_exclusions_1 &lt;- diary %&gt;% filter(is.na(start_date)) %&gt;% select(id, day) Afterwards, I inspect how many NAs there are per row in Table 2.3. Most participants filled out everything, but a couple of them have basically empty rows (e.g., those with 30 or 40 missing values per row). Table 2.3: Count of number of missing values per row na_count_row n 0 1072 1 49 2 16 3 3 4 1 6 1 8 1 9 2 16 1 20 1 23 1 28 2 32 24 39 336 I manually inspected those with more than 15 missing values and neither of them even finished any of the scales. Therefore, Ill exclude them here. # exclude those with &gt; 15 missing values diary_exclusions_2 &lt;- diary %&gt;% mutate( na_count_row = rowSums(is.na(.)) ) %&gt;% filter(na_count_row &gt; 15) %&gt;% mutate(id = droplevels(id)) %&gt;% select(id, day) Alright, time to construct the means for the scales. But first, as always with numbers that participants have to fill in themselves, Ill check for unrealistic entries and general data quality. Ill visualize the text entries for hours and minutes on social media and notifications in Figure 2.2. Figure 2.2: Distribution of self-reported social media indicators For notifications, its hard to argue that its unrealistic that participants thought they got several hundred of them. So I will leave those unchanged. However, there are several outliers in hours_subjective and minutes_subjective. I manually inspected those cases and its hard to tell whether participants just mixed up hours and minutes or whether their entry is indeed not meaningful. One participant filled in 50h and 500min. Another 40h. For those two, Ill set their subjective values to NA. One participant filled in 5h and 86min. The minutes entry might have been a typo, but its hard to tell what the participant meant, so Ill set that to NA as well. diary &lt;- diary %&gt;% mutate( hours_subjective = case_when( hours_subjective &gt; 24 | minutes_subjective &gt; 60 ~ NA_real_, TRUE ~ hours_subjective ), minutes_subjective = if_else( minutes_subjective &gt; 60, NA_real_, minutes_subjective ) ) Next, I check data quality further by seeing whether there are instances of straightlining. For that, I check the variance per daily survey on the well-being scale, need satisfaction, and experiences. diary &lt;- diary %&gt;% rowwise() %&gt;% mutate( well_being_sd = sd( c_across(low_positive_peaceful:low_negative_gloomy), na.rm = TRUE ), needs_sd = sd( c_across(autonomy_1:relatedness_4), na.rm = TRUE ), experiences_sd = sd( c_across(satisfied:enjoyable), na.rm = TRUE ) ) %&gt;% unnest(cols = c()) If we visualize the instances where participants didnt have any variation in their answers (aka straightlining) in Figure 2.3, we see that its not uncommon. There are several daily surveys where participants straightlined. Figure 2.3: Distribution of variances per survey on three scales In general, its not uncommon to select the same answer, especially for well-being measures. However, Id be suspicious of those participants who straightlined for more than one scale (particularly the need satisfaction scale because it has quite a lot of items.) Ill check how many surveys there are that have zero variance across those scales and how many participants there are who have at least one straightlining instance in more than 50% of their surveys. diary &lt;- diary %&gt;% mutate( zero_variances = rowSums( select(., ends_with(&quot;_sd&quot;)) == 0, # count instances of zero variance across the three scales na.rm = TRUE ) ) Overall, Table 2.4 shows that 95% of the sample didnt staightline. Table 2.4: Percentage of the number of zero variance per row zero_variances n percent 0 1440 0.9536424 1 29 0.0192053 2 21 0.0139073 3 20 0.0132450 But there are a couple of participants who seemed to rush through the survey, see Table 2.5. Manually inspecting them indeed shows that its hard to trust their data. (Note that I temporarily remove the rows identified above because we only want to exclude those straightliners with valid data.) Table 2.5: Percentage of participants with a number of zero-variance scales id total_surveys total_straightline_surveys percentage pp_172 4 3 0.75 pp_121 5 3 0.60 pp_164 5 3 0.60 pp_169 5 3 0.60 pp_103 2 1 0.50 pp_168 2 1 0.50 pp_290 2 1 0.50 pp_154 5 2 0.40 pp_198 5 2 0.40 pp_35 5 2 0.40 I will exclude those participants and then surveys with more than one instance of straightlining. diary_exclusions_3 &lt;- c(&quot;pp_172&quot;, &quot;pp_121&quot;, &quot;pp_164&quot;, &quot;pp_169&quot;, &quot;pp_103&quot;, &quot;pp_168&quot;, &quot;pp_290&quot;) Now theres very little straightlining left (see Table 2.6 and compare to Table 2.4). Table 2.6: Percentage of the number of zero variance per row zero_variances n percent 0 1072 0.9571429 1 20 0.0178571 2 14 0.0125000 3 14 0.0125000 Alright, now we can aggregate the individual items to scale scores. Some of those items are reverse-coded. Namely, the third and fourth item for the need satisfaction subscales are reverse coded, as are all negative arousal items (if we want to create a score that shows higher well-being). See codebook for details. # reverse code need items and arousal diary &lt;- diary %&gt;% mutate( across( c(ends_with(&quot;_3&quot;), ends_with(&quot;_4&quot;)), function(.) 8 - . ), across( contains(&quot;negative&quot;), function(.) 6 - . ) ) I create the scales by taking the mean of the respective items. I also create a social_media_subjective measure by combining hours and minutes of estimated social media time. diary &lt;- diary %&gt;% mutate( social_media_subjective = hours_subjective * 60 + minutes_subjective, well_being_state = rowMeans(select(., low_positive_peaceful:low_negative_gloomy), na.rm = TRUE), autonomy_state = rowMeans(select(., contains(&quot;autonomy&quot;)), na.rm = TRUE), competence_state = rowMeans(select(., contains(&quot;competence&quot;)), na.rm = TRUE), relatedness_state = rowMeans(select(., contains(&quot;relatedness&quot;)), na.rm = TRUE) ) Further, participants first day of participation was always a Monday. Specifically, three days in April 2019: the 15th, 22nd, 29th. Lets check whether all of them indeed have their first day on one of those days. Table 2.7 shows all day one surveys which were not recorded on one of those three dates. Indeed, everyone opened their first survey on one of the three start days. Some participants opened/recorded their answers right after midnight, which is fine. Table 2.7: Recorded dates that werent on a Monday id start_date recorded_date duration day pp_21 2019-04-30 00:35:48 2019-04-30 00:57:14 1284 monday pp_61 2019-04-29 16:59:21 2019-05-06 17:00:41 72 monday pp_71 2019-04-22 16:01:29 2019-04-24 11:00:01 154711 monday pp_110 2019-04-22 20:16:35 2019-04-23 00:30:27 15231 monday pp_120 2019-04-23 00:08:15 2019-04-23 00:20:35 739 monday pp_192 2019-04-15 20:36:47 2019-04-16 00:27:47 13859 monday pp_201 2019-04-30 00:27:10 2019-04-30 00:52:59 1548 monday pp_211 2019-04-22 17:24:37 2019-04-23 12:13:59 67761 monday pp_257 2019-04-30 00:52:05 2019-04-30 01:21:55 1789 monday pp_265 2019-04-29 23:28:18 2019-04-30 01:34:43 7584 monday pp_275 2019-04-29 16:20:39 2019-05-06 16:31:47 648 monday However, a handful of participants had their survey open for at least a day. In Table 2.8, we inspect all instances where theres more than one day delay between opening and responding to a survey. Only pp_71 took two days to respond to a survey. Table 2.8: More than one day between begin and response to survey id start_date recorded_date duration day pp_71 2019-04-22 16:01:29 2019-04-24 11:00:01 154711 monday I guess its normal that participants open the survey late, forget it, and fill it out when they next day when they wake up. Because theyre students, waking up might be quite late, so Ill check in Table 2.9 how many of them responded to a survey later than 10am the next day. Two participants were really late (i.e., responded after noon). ## Joining, by = c(&quot;id&quot;, &quot;day&quot;) ## Joining, by = c(&quot;id&quot;, &quot;day&quot;) Table 2.9: Those who filled in the survey the next day id start_date recorded_date duration day pp_53 2019-04-18 16:11:51 2019-04-19 10:08:16 64584 thursday pp_71 2019-04-22 16:01:29 2019-04-24 11:00:01 154711 monday pp_98 2019-04-30 17:21:21 2019-05-01 12:03:12 67310 tuesday pp_161 2019-05-02 18:15:49 2019-05-03 10:05:25 56975 thursday pp_162 2019-04-30 22:00:37 2019-05-01 10:52:15 46297 tuesday pp_211 2019-04-22 17:24:37 2019-04-23 12:13:59 67761 monday pp_211 2019-04-23 23:40:53 2019-04-24 10:30:27 38972 tuesday pp_244 2019-04-19 18:45:44 2019-04-20 11:52:12 61587 friday I manually checked the data from all those participants who filled out surveys so late that two surveys were recorded on the same day. pp_71 responded to the Monday survey on Wednesday, in addition to the regular Wednesday survey. For that reason, we cant know whether day one refers to Monday or Tuesday. All other participants are fine: They sometimes recorded their response late, but their response pattern shows no indication that they couldnt refer to a clear day in their response. Therefore, I remove the day one survey of pp_71 because we cant tell to which day it refers, but leave the rest as is. diary_exclusions_4 &lt;- tibble(id = &quot;pp_71&quot;, day = &quot;monday&quot;) 2.3 Screen time data Load the screen time data. screen_time_raw &lt;- read_csv(here(&quot;data&quot;, &quot;study1&quot;, &quot;screen_time_raw.csv&quot;)) Alright, the data are extremely wide because of the Qualtrics format. There are 220 variables in total. The data look like this: Total time over the past week of social media, plus total time per day (Q2_1 to Q10_2) Typing in the top ten social networking media (Q4_1 to Q4_10) Then participants filled in 13 items for each of those ten apps, first total time and notifications for the week for that app, then hours and minutes per day for that app (total 130 items, Q3_1 to Q76_2) Then participants reported six items per each of the ten apps, the first of which was overall pickups in the past week, followed by pickups for each day of the week (Q17_1_TEXT to Q17_10_5) Before turning the data into long format, I need to give some sensible variables names. However, manually renaming all variables is a real pain. So Ill create the names systematically and store them in a vector, then assign them to the variables. # define days days &lt;- c( &quot;monday&quot;, &quot;tuesday&quot;, &quot;wednesday&quot;, &quot;thursday&quot;, &quot;friday&quot; ) # the list of apps apps &lt;- paste0( rep(&quot;app_&quot;, 10), 1:10 ) # add hours and minutes to apps apps_times &lt;- paste0( rep(apps, each = 6*2), &quot;_&quot;, c(&quot;hours&quot;, &quot;minutes&quot;) ) # the overall time per week and day names1 &lt;- paste0( rep(c(&quot;hours_total_&quot;, &quot;minutes_total_&quot;),6), rep(c(&quot;week&quot;, days), each = 2) ) # now app, minutes, hours, per day names2 &lt;- paste0( apps_times, &quot;_&quot;, rep(c(&quot;week&quot;, days), each = 2) ) # notifications per day names3 &lt;- paste0( apps, &quot;_&quot;, &quot;notifications&quot;, &quot;_week&quot; ) # pickups per week and day names4 &lt;- paste0( rep(apps, each = 6), &quot;_pickups_&quot;, rep(c(&quot;week&quot;, days), 6) ) Then I apply those names. Once more, I use a new object and dont overwrite the raw data. # let&#39;s rename screen_time &lt;- screen_time_raw %&gt;% # meta-data rename( start_date = StartDate, end_date = EndDate, progress = Progress, duration = Duration__in_seconds_, finished = Finished, recorded_date = RecordedDate, id = study_id ) %&gt;% # the overall time per week and day rename_with( ~ names1, Q2_1:Q10_2 ) %&gt;% # the top ten apps rename( app_ = Q4_1:Q4_10 ) %&gt;% # now rename all app minutes and hours per day rename_with( ~ names2, c( # except the notifications because they don&#39;t fit the pattern Q3_1:Q76_2, -Q11_1, -Q22_1, -Q78_1, -Q29_1, -Q43_1, -Q36_1, -Q50_1, -Q57_1, -Q64_1, -Q71_1 ) ) %&gt;% # the past week notifications per app rename_with( ~ names3, c( Q11_1, Q22_1, Q78_1, Q29_1, Q43_1, Q36_1, Q50_1, Q57_1, Q64_1, Q71_1 ) ) %&gt;% # rename pickup variables rename_with( ~ names4, Q17_1_TEXT:Q17_10_5 ) %&gt;% # only keep variables of interest select( -Status ) # remove name objects (they were only temp files) rm(apps, apps_times, days, names1, names2, names3, names4) Before I assign variables types and factor levels, I saw that, for some reason, the past weeks number of pickups for each app have text entries, not numerical (i.e., \"twenty one\" instead of 21). I know that the english package can translate numbers to words, but not the other way around. Thank god, someone has written a function for this, see full details here. The function is in the Setting up chapter. Note that this word2num function doesnt work well with the word and. For example, \"three hundred and eleven\" turns into 611, but \"three hundred eleven\" turns into the correct 311. Therefore, I first remove all ands from those variables, then apply the function. Similarly, Ill replace all a with one (e.g., a hundred to one hundred). One participant also replied with a string (that they access facebook via their browser), so Ill set that to NA. screen_time &lt;- screen_time %&gt;% mutate( across( ends_with(&quot;pickups_week&quot;), ~ case_when( .x == &quot;NA because accessed Facebook through Safari&quot; ~ NA_character_, .x == &quot;NA because accessed Twitter through Safari&quot; ~ NA_character_, TRUE ~ .x ) ), across( ends_with(&quot;pickups_week&quot;), ~ str_remove(.x, &quot;and&quot;) ), across( ends_with(&quot;pickups_week&quot;), ~ str_replace(.x, &quot;A &quot;, &quot;one &quot;) ) ) %&gt;% rowwise() %&gt;% mutate( across( ends_with(&quot;pickups_week&quot;), word2num ), across( ends_with(&quot;pickups_week&quot;), unlist ) ) %&gt;% unnest(cols = c()) Next, I assign correct variable types and factor levels. screen_time &lt;- screen_time %&gt;% mutate( id = paste0(&quot;pp_&quot;, id), across( # turn those two into factors c(finished, id), as.factor ), finished = fct_recode( finished, &quot;yes&quot; = &quot;1&quot;, &quot;no&quot; = &quot;0&quot; ) ) Then I exclude empty rows. The data set has all participant numbers even when they didnt participate, so Ill identify those without a start date as well as those who didnt fill in app_1, which means they have empty rows other than the meta-data. screen_time_exclusions_1 &lt;- screen_time %&gt;% filter(is.na(start_date) | is.na(app_1)) %&gt;% mutate(id = droplevels(id)) %&gt;% pull(id) At this point, the data set has measures on two levels: per app and time frame (day, full week), and the total times across all apps per day. I think its easiest to separate the data. The per app data are only informative for plotting and descriptives, but the actual data were interested in are the total times summed across all apps. So Ill first get the per data app (excluding empty rows temporarily from screen_time). Note that I add the names of the apps in a separate step and then calculate the objective social media time. # turn app data long apps_long &lt;- screen_time %&gt;% filter(!is.na(start_date)) %&gt;% filter(!is.na(app_1)) %&gt;% mutate(id = droplevels(id)) %&gt;% select( start_date:id, app_1_hours_week:app_10_pickups_friday ) %&gt;% pivot_longer( cols = c(app_1_hours_week:app_10_pickups_friday), names_to = c( &quot;app&quot;, &quot;rank&quot;, &quot;measure&quot;, &quot;time_frame&quot; ), names_sep = &quot;_&quot; ) %&gt;% pivot_wider( # then spread the measure variable which now contains hours, minutes, notifications, pickups names_from = &quot;measure&quot;, values_from = &quot;value&quot; ) %&gt;% select(-app) # redundant # get the names of the apps app_names &lt;- screen_time %&gt;% filter(!is.na(start_date)) %&gt;% filter(!is.na(app_1)) %&gt;% mutate(id = droplevels(id)) %&gt;% select( id, app_1:app_10 ) %&gt;% pivot_longer( -id, names_to = c(&quot;app&quot;, &quot;rank&quot;), names_sep = &quot;_&quot; ) %&gt;% select(-app) %&gt;% # redundant rename( app = value # contains name of app ) # add them to the long data set apps_long &lt;- left_join( apps_long, app_names, by = c(&quot;id&quot;, &quot;rank&quot;) ) %&gt;% relocate( app, .after = rank ) %&gt;% # if someone only filled in minutes, but not hours, turn hours to 0 mutate( hours = case_when( is.na(hours) &amp; is.na(minutes) ~ NA_real_, is.na(hours) &amp; !is.na(minutes) ~ 0, TRUE ~ hours ), # when someone only filled in hours, but not minutes, turn minutes to 0 minutes = case_when( is.na(hours) &amp; is.na(minutes) ~ NA_real_, !is.na(hours) &amp; is.na(minutes) ~ 0, TRUE ~ minutes ), # total time social_media_objective = hours * 60 + minutes ) Then I get the total times across all apps in the long format. Note that participants reported their screen time on the weekend. Thus, their reports for week will not be identical with the sum of Monday to Friday, because the screen time feature measures time on the weekend as well (aka additional screen time measured from the end of Friday until the moment they reported their weekly screen time). # get total screen time (of all social media apps in long format) totals_long &lt;- screen_time %&gt;% filter(!is.na(start_date)) %&gt;% filter(!is.na(app_1)) %&gt;% mutate(id = droplevels(id)) %&gt;% select( id, hours_total_week:minutes_total_friday ) %&gt;% pivot_longer( -id, names_to = c( &quot;measure&quot;, &quot;total&quot;, &quot;time_frame&quot; ), names_sep = &quot;_&quot; ) %&gt;% select(-total) %&gt;% # redundant pivot_wider( # spread hours and minutes names_from = &quot;measure&quot;, values_from = &quot;value&quot; ) %&gt;% # same as above: if one of the two measures (hours or minutes) has an entry and the other is NA, we set that NA to 0 mutate( hours = case_when( is.na(hours) &amp; is.na(minutes) ~ NA_real_, is.na(hours) &amp; !is.na(minutes) ~ 0, TRUE ~ hours ), # when someone only filled in hours, but not minutes, turn minutes to 0 minutes = case_when( is.na(hours) &amp; is.na(minutes) ~ NA_real_, !is.na(hours) &amp; is.na(minutes) ~ 0, TRUE ~ minutes ), # total time social_media_objective = hours * 60 + minutes ) The totals_long is the data set of interest because it has total screen time per day, which we can match with the diary data. Thus, to arrive at the final sample size, well only keep participants who have data in the diary data set. For merging, well need to exclude those participants in totals_long who dont have a match in diary. But first, I check how much the sum of total time across apps per day in total_long converges with the sum of time across all apps per day in apps_long. There will definitely be differences, simply because participants have more chances to make typos when reporting the individual apps per day (in apps_long) compared to reporting the overall time across all apps once per day (in total_long). Also, participants will occasionally skip an individual app per day rating. Below, I get the total time per day across all ten apps and then join that estimate with the data set containing the total times (total_long). Afterwards, per day, I see how much the two variables differ. # get total time across apps per day apps_aggregated &lt;- apps_long %&gt;% group_by(id, time_frame) %&gt;% filter(time_frame != &quot;week&quot;) %&gt;% # no week summarise(total_time = sum(social_media_objective, na.rm = TRUE)) # join with total times compare &lt;- left_join( totals_long %&gt;% filter(time_frame != &quot;week&quot;), apps_aggregated ) %&gt;% # and compute a deviance score mutate( difference = social_media_objective - total_time ) Overall, the two sums of use across apps and days are highly similar, with a mean difference of M = 4.09, SD = 17.4 and an extremely high correlation, r = 0.98. There were some missing values in the total social media time per day that participants reported. I check who has missing data on any of the weekdays in the totals_long data set. We can check whether those participants have reported individual app use for that day and use an aggregate over apps on that day as a substitute, given their high correlation. In other words: When participants forgot to report their overall social media use for a day, but reported use for all social media apps they used on that day, we can just use the sum of those social media apps as a replacement for overall social media use. However, we see in 2.10 that all participants reported social media screen time across all days, so no need for imputation. Table 2.10: Amount of missing values in data comparing total screen time id time_frame hours minutes social_media_objective total_time difference 0 0 0 0 0 0 0 Before I merge all data sets, I need to add the objective notifications and pickups (currently in apps_long) to the totals_long data set. Note that at the time of the study, iOS screen time wouldnt report notifications for app per day, only for the past week. Therefore, the weekly notifications have the same caveat as the weekly screen time: they will include a couple of notifications that participants received on the day they retrospectively reported their objective use. For pickups, as with screen time, we can sum up all days to get an accurate measure of weeks pickups. Ill sum up both pickups and notifications over participant. Pickups will be per day, but notifications only per week. I then add pickups to the totals_long data set. Notifications Ill add later during merging (because they are not aggregated per day). # get total pickups across apps per day pickups_aggregated &lt;- apps_long %&gt;% group_by(id, time_frame) %&gt;% filter(time_frame != &quot;week&quot;) %&gt;% # no week filter(!is.na(pickups)) %&gt;% # exclude missing values (if I do na.rm = T in sum below it will result in zeros to sum NAs) summarise(pickups_objective = sum(pickups)) # get notifications across apps per participant notifications_aggregated &lt;- apps_long %&gt;% group_by(id) %&gt;% summarise(weekly_notifications = sum(notifications, na.rm = TRUE)) # add pickups to totals_long totals_long &lt;- left_join( totals_long, pickups_aggregated, by = c(&quot;id&quot;, &quot;time_frame&quot;) ) 2.4 Merge data sets Finally, we can merge the three data sets. Before I do any merging, Ill check out the number of data points per data set and the ones they have in common before I count exclusions. This is mostly for reporting in the paper. # number of participants who took initial survey (take out empty rows) personality %&gt;% filter(!id %in% personality_exclusions_1) %&gt;% pull(id) -&gt; personality_n length(personality_n) ## [1] 292 # number of those who also participated in diary part diary %&gt;% anti_join(., diary_exclusions_1) -&gt; diary_n ## Joining, by = c(&quot;id&quot;, &quot;day&quot;) length(unique(diary_n$id)) ## [1] 275 # number of those who came back to report screen time screen_time %&gt;% filter(!id %in% screen_time_exclusions_1) %&gt;% pull(id) -&gt; screen_time_n length(screen_time_n) ## [1] 97 The diary data set is the base because it has our variables of interest and the correct time structure. To that data set we add the objective social media measures from the totals_long data set. The identifier for a survey are the participant id and the day on which they filled out a survey/for which they reported objective use. Note that we omit the meta-data from totals_long (e.g., start_date) because they are different from the meta-data of diary. We also only keep the variables of interest from here on, meaning Ill exclude some variables that arent informative. study1 &lt;- left_join( # the diary data with only relevant variables diary %&gt;% select( duration_diary = duration, # so that we can get filling out times for each data set id:day, pickups_subjective:enjoyable, # don&#39;t need individual minutes and hours anymore social_media_subjective:relatedness_state ), # the objective totals with only relevant variables totals_long %&gt;% filter(time_frame != &quot;week&quot;) %&gt;% rename( day = time_frame ) %&gt;% select(id, day, social_media_objective, pickups_objective), # joining by the two identifiers by = c(&quot;id&quot;, &quot;day&quot;) ) The merging above kept all cases of diary and added variables from totals_long when there was a match, otherwise setting the new variables to NA. Therefore, I remove all cases which have NA on the added variables to get to our final sample. (There were no missings in the social_media_objective variable.) In other words, this removes those participants who didnt fill out the screen time survey. study1 &lt;- study1 %&gt;% filter(!is.na(social_media_objective)) For descriptive information, Ill also add the survey duration of the screen time data. study1 &lt;- left_join( study1, screen_time %&gt;% select(id, duration) %&gt;% rename( duration_screen_time = duration ), by = &quot;id&quot; ) Okay, then I add the personality variables. Again, study1 is the master and we add personality variables (aka traits from personality), omitting variables that arent of interest. study1 &lt;- left_join( study1, personality %&gt;% select( duration_personality = duration, id, bpns_1:openness ), by = &quot;id&quot; ) Then, I add the notifications over one week as a constant. study1 &lt;- left_join( study1, notifications_aggregated, by = &quot;id&quot; ) Alright, only keeping those who had screen time measures already removed a large number of cases, some of them were part of the exclusions for both the personality and the diary parts. I now apply all exclusions (if necessary) to get to the final data set. Note that I dont exclude those in diary_exclusions_2 after all: Those are the ones who didnt fill in an experience sampling survey, but all of them still have personality scores and the retrospective screen time estimates. ## personality # nobody left who didn&#39;t fill in the personality survey unique(study1$id %in% personality_exclusions_1) ## [1] FALSE # and nobody with empty rows unique(study1$id %in% personality_exclusions_2) ## [1] FALSE # the people who failed attention checks either didn&#39;t fill out the diary or the screen time part unique(study1$id %in% personality_exclusions_3) ## [1] FALSE # same goes for personality straightliners unique(study1$id %in% personality_exclusions_4) ## [1] FALSE ## diary # some people with empty diary entries left, nobody left who didn&#39;t participate in the diary (no wonder, we used diary as the base for merging) # those people will have empty diary entries, but still data for screen time, so no need to exclude them here, they&#39;ll be dropped from analysis automatically because of NA inner_join(diary_exclusions_1, study1 %&gt;% select(id, day)) %&gt;% nrow() ## Joining, by = c(&quot;id&quot;, &quot;day&quot;) ## [1] 33 # don&#39;t exclude anyone who missed a diary entry # diary straightliners: need to exclude one person study1$id[study1$id %in% diary_exclusions_3] ## [1] pp_169 pp_169 pp_169 pp_169 pp_169 ## 303 Levels: pp_1 pp_10 pp_100 pp_101 pp_102 pp_103 pp_104 pp_105 pp_106 pp_107 pp_108 pp_109 pp_11 pp_110 pp_111 ... pp_NA study1 &lt;- study1 %&gt;% filter(!id %in% diary_exclusions_3) # last, exclude one diary entry that was provided too late (see details above) study1 &lt;- study1 %&gt;% anti_join(., diary_exclusions_4) ## Joining, by = c(&quot;id&quot;, &quot;day&quot;) ## screen time exclusions # those with empty rows will have been excluded as we merged diary with totals_long (totals long excluded those who had all NA) unique(study1$id %in% screen_time_exclusions_1) ## [1] FALSE Good, then we have our final data set. Because screen time was filled in retrospectively in the third session, those who didnt fill in an experience sample will have NAs on the self-reports, but still a row with data because screen time for that day is there. So each person has five rows, except for that one participant who filled in one of the surveys too late. Next, I create an accuracy score for the comparison of objective and subjective phone use indicators. The objective measure is our true estimate. We want to know how far off participants were, relative to that true value, in their estimate. For that, we calculate the percentage error by subtracting the objective value from the subjective value (to get difference) and dividing by the objective value (to get error percent). Afterwards, we multiply by 100 to get a percentage. Say I estimated 110 subjective minutes, but the true value (i.e., the objective minutes) is 100. Then the error is 10%, an overestimate. If I estimated 100 subjective minutes, but the true value is 110, the error is ~9%, an underestimate. In my opinion, that is more intuitive than a difference score. Its easier to understand the magnitude of an error in percent (say 80%) than raw units (say 10 minutes). However, percent error brings one complication with it: If participants estimated 0 social media use, percent error becomes meaningless. 0 - objective / objective will always lead -100%. Theres 11 cases with an estimate of zero, but actual, objective use. Its impossible in this case to determine an error percentage, simply because 0 is meaningless in the formula. I could use a difference score instead, but thatll make the interpretation less intuitive. Those zeros mightve also been typos, given how rare zeros are in the sample. I could set those error instances to missing, but in the analysis, we want to be able to compare coefficient predicting both social media and error. Therefore, Ill set all three variables to NA. study1 &lt;- study1 %&gt;% mutate( # error social media time error = (social_media_subjective - social_media_objective) / social_media_objective * 100, across( c(social_media_subjective, social_media_objective, error), ~ if_else(error == -100, NA_real_, .x) ) ) Lets inspect the distribution in Figure 2.4. We see that a couple of participants have massive overestimation errors. Figure 2.4: Distribution of percent error for screen time I checked their raw data: One participant indicated exactly five hours, but probably meant five minutes (i.e., the participant with an error of 1.49^{4}. Similarly, the second largest entry seems to be a typo (i.e., 58 minutes). Ill set those all entries that are larger than 1500% to NA. Also, one participant had zero objective time and zero subjective time. Dividing by zero to get error above leads to NaN, so Ill set that NA. study1 &lt;- study1 %&gt;% mutate( error = if_else(error &gt; 1500, NA_real_, error), error = if_else( social_media_subjective == 0 &amp; social_media_objective == 0, NA_real_, error ) ) Last, some housekeeping, reordering variables and making sure they all have the correct variable type, plus reordering the day factor. study1 &lt;- study1 %&gt;% select( id, age:ethnicity, gender, starts_with(&quot;duration&quot;), day, social_media_subjective, pickups_subjective, notifications_subjective, social_media_objective, error, pickups_objective, weekly_notifications, well_being_state:relatedness_state, low_positive_peaceful:enjoyable, autonomy_trait:openness, bpns_1:big_five_45 ) %&gt;% mutate( id = droplevels(id), day = as.factor(day), day = fct_relevel( day, &quot;monday&quot;, &quot;tuesday&quot;, &quot;wednesday&quot;, &quot;thursday&quot;, &quot;friday&quot; ) ) When plotting the app data I want them to reflect the final sample, which is why I only keep those participants and surveys in the apps_long data set that are also in the final data set (i.e., study1). I cant just exclude those participants in apps_long because I also excluded surveys in diary on which study1 is based. Therefore, I only keep those entries in apps_long which have a corresponding row in study1 (aka entries in both id and day). Also, Ill remove week as a time frame and add the notifications for that app over whole week as a constant per app. apps_week &lt;- apps_long %&gt;% select( id, time_frame, app, rank, notifications ) %&gt;% filter(time_frame == &quot;week&quot;) %&gt;% select(-time_frame) apps_long &lt;- left_join( # only those participants and days that are in final data set study1 %&gt;% select(id, day), # and the rest of the variables from apps_long, but without week apps_long %&gt;% rename(day = time_frame) %&gt;% filter(day != &quot;week&quot;) %&gt;% select(-notifications), by = c(&quot;id&quot;, &quot;day&quot;) ) %&gt;% # add the notifications for that app per week as a constant left_join( ., apps_week, by = c(&quot;id&quot;, &quot;app&quot;, &quot;rank&quot;) ) %&gt;% select(id, day, rank:notifications) %&gt;% rename(notifications_per_week = notifications) Last, there are some NaNs in the data set after the transformations. Ill turn them into missings (i.e., NA). study1 &lt;- study1 %&gt;% mutate( across( everything(), ~ na_if(.x, &quot;NaN&quot;) ) ) "],["descriptives-and-visualizations-study-1.html", "3 Descriptives and Visualizations Study 1 3.1 Overview 3.2 Meta-level 3.3 Person-level 3.4 App-level 3.5 Day level 3.6 Plots for paper", " 3 Descriptives and Visualizations Study 1 3.1 Overview In this section, I describe and visualize the sample and variables. We have variables on the meta-level (about the survey), the person-level, the app-level, and the day-level. App-level data is in the apps_long data file; all other in the study1 data file. Meta-level Duration of the entry survey, when participants reported traits (duration_personality) Duration of the exit survey, when participants reported their screen time (duration_screen_time) Person-level Participant identifier (id) age in years ethnicity Notifications of social media apps over the past week (weekly_notifications) Basic Psychological Need Satisfaction (autonomy_trait, competence_trait, relatedness_trait) plus their individual items (starting with bpns_) Big Five (extraversion, agreeableness, conscientiousness, neuroticsim, openness) plus their individual items (starting with big_five_) App-level What app participants report use for (app) On what rank was that app on participants top ten (rank) Notifications for that app for the week (notifications_per_week) Pickups for that app on that day (pickups) Screen time for that app on that day (social_media_objective) Day-level Duration of the survey on that day (duration_diary) day the survey was answered Estimated time on social media on that day (social_media_subjective) Estimated pickups of social media apps on that day (pickups_subjective) Estimated notifications of social media apps on that day (notifications_subjective) Objective time on social media on that day (social_media_objective) Objective pickups of social media apps on that day (pickups_objective) Well-being on that day (well_being) plus its individual items (starting with low_ and high_) Basic psychological needs on that day (autonomy_state, competence_state, relatedness_state) plus their individual items (starting with autonomy_, competence_, relatedness_ respectively) Experiences of satisfaction, boredom, stress, enjoyment on that day (satisfied, boring, stressful, enoyable) 3.2 Meta-level I begin with describing and plotting the duration of the entry and exit surveys. Table 3.1 shows descriptive stats; Figure 3.1 shows that twp participants had their entry surveys open for a day before pressing send, which skews the mean massively. However, those peoples data look good, so I wouldnt exclude them here. Note: Colors are from here. Table 3.1: Duration of entry and exit surveys variable mean sd median min max range cilow cihigh duration_personality 1H 5M 56S 4H 36M 55S 15M 40S 1M 22S 1d 9H 33M 36S 1d 9H 32M 14S 9M 50S 2H 2M 3S duration_screen_time 20M 21S 20M 8S 13M 18S 1M 8S 1H 51M 29S 1H 50M 21S 16M 17S 24M 26S Figure 3.1: Duration of surveys 3.3 Person-level Lets have a look at the final sample. Overall, our sample size is N = 96. The sample has a mean age of M = 20, SD = 1. The sample consists mostly of women (66 women, 30 men, and one non-binary participant). Most participants are Asian, followed by White, Black, and Hispanic, see Table 3.2 Table 3.2: Ethnicity of the sample ethnicity count percent Asian 40 42 White 26 27 Black or African American 11 11 Hispanic or Latino 10 10 Multiracial 6 6 NA 2 2 Native Hawaiian or Other Pacific Islander 1 1 Alright, next we look at the objective count of notifications over the past week, aggregated across all apps. Table 3.3 shows that participants received quite a lot of notifications from social media apps only. That distribution is heavily skewed (Figure 3.2 by a couple of participants who received several thousand notifications over the week. Table 3.3: Weekly notifications (objective) across all apps variable mean sd median min max range cilow cihigh weekly_notifications 997.6354 673.4178 868.5 126 3382 3256 861.1883 1134.083 Figure 3.2: Weekly notifications (objective) across all apps Now we look at the trait variables: the basic psychological need satisfaction and the big five. Note that I follow recent recommendations and calculate \\(\\omega\\) for reliability. Table 3.4 shows the descriptive information of the three psychological needs and the big five. Figure 3.3 shows their distribution. The sample isnt too large, so considering the small size, Id say everything looks pretty good. Table 3.4: Descriptives for trait variables variable mean sd median min max range cilow cihigh omega autonomy_trait 4.51 0.88 4.50 1.25 6.88 5.62 4.33 4.68 0.82 competence_trait 5.16 0.70 5.31 3.50 6.50 3.00 5.01 5.30 0.75 relatedness_trait 4.54 1.03 4.56 2.38 6.62 4.25 4.33 4.75 0.88 extraversion 3.13 0.74 3.00 1.62 5.00 3.38 2.98 3.29 0.86 agreeableness 3.68 0.56 3.59 2.33 5.00 2.67 3.57 3.80 0.75 conscientiousness 3.50 0.58 3.50 2.00 4.89 2.89 3.39 3.62 0.75 neuroticism 3.29 0.50 3.38 2.00 4.38 2.38 3.19 3.39 0.59 openness 3.54 0.48 3.60 2.10 4.80 2.70 3.45 3.64 0.66 Figure 3.3: Distribution of trait variables In Figure 3.4 we see the correlations between those traits. As expected psychological needs are correlated highly with each other. Credit for the lm lines goes to data prone, whose idea I adapted. Figure 3.4: Correlation matrix of trait level variables 3.4 App-level First, Figure 3.5 shows what apps mostly nominated (i.e., used). We see that out of the sample, most participants had Messaging, Snapchat, Whatsapp etc. as part of their top ten. Figure 3.5: Percentage of nominated apps Next, I visualize how many minutes each of those apps was used across the sample. For that, I need to reshape the data a bit to get the mean minutes per app across all days and all participants. In Figure 3.6, I show the average objective time per app. Note that the CIs are across the entire data and not nested by app or day. Also, a high mean doesnt mean that much because it could just be from one participant who used it a lot on two days. The size of the points shows how often an app was reported across the entire sample. For apps that only had one entry, those CI will be nonexistent. In addition, I now exclude entries on social_media_objective that have NA. The NA here can mean participants just didnt fill in anything, or they had zero duration on that day. Because adding up the raw scores across apps was so close to the daily total, Ill exclude NAs here. Figure 3.6: Average daily objective time for all apps across participants and days Ill do the same for objective pickups per app, averaged across day and participant. Figure 3.7 shows that the same apps that got a lot of screen time had a lot of pickups. Figure 3.7: Average daily pickups for all apps across participants and days Last, I check which apps got the most notifications over the week in Figure 3.8, on average. Its interesting to see that Facebook had a lot of screen time and pickups, but much fewer notifications. Also, these notifications are per week, and not per day, as the previous two figures. Figure 3.8: Average notifications per week for all apps across participants 3.5 Day level Alright, were at the most interesting section, the daily surveys. I first look at how long people typically took for a survey. Table 3.5 shows that the mean is highly skewed because of outliers and the median more appropriate to describe the duration. In Figure 3.9 we see that a couple of people took a long time from opening to submitting the survey. I checked those participants who took a long time in the data processing section. The maximum duration here from someone who didnt open the survey on a Friday. So that duration is just the survey closing automatically after two days, which really drives up the mean. Table 3.5: Duration of daily surveys variable mean sd median min max range cilow cihigh duration_diary 50M 22S 2H 44M 17S 16M 0S 25S 2d 0H 27M 48S 2d 0H 27M 23S 35M 5S 1H 5M 40S Figure 3.9: Duration of daily surveys Alright, next I inspect overall response rate in the final sample, aka how many valid surveys do we have among the final sample. Each participant received five surveys, one for each day, so 96 participants x 5 = 480. We have 435 surveys in the final sample where participants actually responded, which means a 91% response rate among the final sample. Lets inspect response rate per day. As is to be expected, participants lost motivation over the course of the week. However, even the response rate on Friday is really high (at least among our sample of valid responses). We should still consider to take the day grouping into account when modelling the data later in the analysis. Figure 3.10: Survey responses per day Next, I describe and plot the distributions of the social media use variables. The distribution and CI is of the entire sample, not aggregated by participant or day first. Table 3.6 shows that participants werent too far off in their estimates, which is interesting. As expected (Figure 3.11), the social media variables are a bit skewed, but overall, they look fine. Table 3.6: Descriptive information on social media variables variable mean sd median min max range cilow cihigh social_media_subjective 153 112 130 5 692 687 143 164 social_media_objective 139 94 118 2 565 563 130 148 error 46 156 -3 -96 1186 1282 31 61 pickups_subjective 34 43 17 0 259 259 30 38 pickups_objective 49 31 44 0 196 196 46 52 notifications_subjective 61 99 30 0 700 700 52 71 Figure 3.11: Distribution of social media variables I also want to see how much variability there is between the objective and subjective measures. In Figure 3.12 we see per participant the difference between objective and subjective social media use. The numbers in the grey box show whether the subjective report is an underestimate (negative number) or overestimate (positive number). Inspiration for the plot from here and here. Figure 3.12: Difference between subjective and objective social media use (difference in grey box) Now lets look at the state well-being and psychological needs variables plus the four experiences (e.g., boring). Again, I calculate \\(\\omega\\), but this time for the entire sample in Table 3.7. That will necessarily bias the estimate because theres multiple measures per person. Im not aware of a consensus reliability procedure for repeated measures. Figure 3.13 shows that the data look pretty good. Table 3.7: Descriptives for state variables variable mean sd median min max range cilow cihigh omega well_being_state 3.23 0.70 3.17 1.25 5 3.75 3.17 3.30 0.85 autonomy_state 4.50 1.16 4.25 1.00 7 6.00 4.39 4.60 0.69 competence_state 4.60 1.27 4.50 1.00 7 6.00 4.48 4.72 0.79 relatedness_state 5.33 1.07 5.50 2.50 7 4.50 5.23 5.43 0.70 satisfied 4.61 1.33 5.00 1.00 7 6.00 4.49 4.74 NA boring 3.40 1.55 3.00 1.00 7 6.00 3.25 3.55 NA stressful 3.96 1.81 4.00 1.00 7 6.00 3.79 4.13 NA enjoyable 4.27 1.41 4.00 1.00 7 6.00 4.14 4.40 NA Figure 3.13: Distribution of state variables In Figure 3.14 we see the correlations between variables on the state level. Figure 3.14: Correlation matrix of state level variables. social = screen time on social media; _s = subjective; _o = objective; not = notifications In Figure 3.15 we see the correlations between use variables on the state level and the trait level. Figure 3.15: Correlation matrix of use variables (state) and personality traits. social = screen time on social media; _s = subjective; _o = objective; not = notifications; extra = extraversion; agree = agreeableness; con = conscientiousness; neuo = neuroticism; open = openness 3.6 Plots for paper Here, Ill create summary figures for the paper. Ill begin with plotting the traits. For the plot, the data need to be in the long format. dat &lt;- study1 %&gt;% group_by(id) %&gt;% slice(1) %&gt;% ungroup() %&gt;% select(all_of(c(&quot;id&quot;, trait_descriptives$variable))) %&gt;% pivot_longer( -id, names_to = &quot;variable&quot;, values_to = &quot;value&quot; ) rename_levels &lt;- c( &quot;Autonomy&quot;, &quot;Competence&quot;, &quot;Relatedness&quot;, &quot;Agreeableness&quot;, &quot;Conscientiousness&quot;, &quot;Extraversion&quot;, &quot;Neuroticism&quot;, &quot;Openness&quot; ) my_string &lt;- &quot;_trait&quot; # reorder and rename factor levels clean_plot_data &lt;- function( dat, levels_to_rename, string_to_remove ){ dat &lt;- dat %&gt;% mutate( # in case it&#39;s social media variables variable = case_when( variable == &quot;social_media_objective&quot; ~ &quot;Objective (h)&quot;, variable == &quot;social_media_subjective&quot; ~ &quot;Subjective (h)&quot;, variable == &quot;error&quot; ~ &quot;Accuracy (%)&quot;, TRUE ~ variable ), # remove _trait at the end and capitalize variable = str_to_sentence(str_remove(variable, string_to_remove)), variable = as.factor(variable), variable = str_replace(variable, &quot;_&quot;, &quot;-&quot;), # reorder factor levels variable = fct_relevel( variable, levels_to_rename ) ) return(dat) } dat &lt;- clean_plot_data(dat, rename_levels, my_string) trait_descriptives &lt;- clean_plot_data(trait_descriptives, rename_levels, my_string) Okay, we already have the aggregated info in trait_descriptives, so we can get to plotting. # function for breaks my_breaks &lt;- function(x) { if (max(x) &gt; 5){ 1:7 } else { 1:5 } } # function for limits my_limits &lt;- function(x) { if (max(x) &gt; 5){ c(1,7) } else { c(1,5) } } # color palette cb_palette &lt;- c(&quot;#000000&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # plot ggplot( dat, aes( x = value, y = 1, color = variable, fill = variable ) ) + geom_quasirandom(groupOnX=FALSE, size = 0.3, alpha = 0.5) + facet_wrap( ~ variable, scales = &quot;free_x&quot; ) + scale_x_continuous(breaks = my_breaks, limits = my_limits) + geom_text( data = trait_descriptives, aes( x = 1.6, y = 1.4, label = paste0(&quot;M = &quot;, mean), family = &quot;Corbel&quot; ), size = 2.5, color = &quot;black&quot; ) + geom_text( data = trait_descriptives, aes( x = 1.6, y = 1.3, label = paste0(&quot;SD = &quot;, sd), family = &quot;Corbel&quot; ), size = 2.5, color = &quot;black&quot; ) + geom_text( data = trait_descriptives, aes( x = 1.6, y = 1.2, label = paste0(&quot;\\u03a9 = &quot;, omega), family = &quot;Corbel&quot; ), size = 2.5, color = &quot;black&quot; ) + theme_cowplot() + scale_colour_manual(values=cb_palette) + scale_fill_manual(values = cb_palette) + theme( axis.text.y = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank(), strip.background.x = element_blank(), strip.background.y = element_blank(), legend.position = &quot;none&quot;, text = element_text(family = &quot;Corbel&quot;) ) -&gt; figure1 figure1 ggsave( here(&quot;figures&quot;, &quot;figure1.tiff&quot;), plot = figure1, width = 21 * 0.8, height = 29.7 * 0.4, units = &quot;cm&quot;, dpi = 300 ) Okay, next the state variables. dat &lt;- study1 %&gt;% select(all_of(c(&quot;id&quot;, state_descriptives$variable))) %&gt;% pivot_longer( -id, names_to = &quot;variable&quot;, values_to = &quot;value&quot; ) rename_levels &lt;- c( &quot;Autonomy&quot;, &quot;Competence&quot;, &quot;Relatedness&quot;, &quot;Boring&quot;, &quot;Enjoyable&quot;, &quot;Satisfied&quot;, &quot;Stressful&quot;, &quot;Well-being&quot; ) my_string &lt;- &quot;_state&quot; dat &lt;- clean_plot_data(dat, rename_levels, my_string) state_descriptives &lt;- clean_plot_data(state_descriptives, rename_levels, my_string) Then to plotting. # plot ggplot( dat, aes( x = value, y = 1, color = variable, fill = variable ) ) + geom_quasirandom(groupOnX=FALSE, size = 0.1, alpha = 0.5) + facet_wrap( ~ variable, scales = &quot;free_x&quot; ) + scale_x_continuous(breaks = my_breaks, limits = my_limits) + geom_text( data = state_descriptives, aes( x = 1.6, y = 1.4, label = paste0(&quot;M = &quot;, mean), family = &quot;Corbel&quot; ), size = 2.5, color = &quot;black&quot; ) + geom_text( data = state_descriptives, aes( x = 1.6, y = 1.3, label = paste0(&quot;SD = &quot;, sd), family = &quot;Corbel&quot; ), size = 2.5, color = &quot;black&quot; ) + geom_text( data = state_descriptives, aes( x = 1.6, y = 1.2, label = paste0(&quot;\\u03a9 = &quot;, omega), family = &quot;Corbel&quot; ), size = 2.5, color = &quot;black&quot;, alpha = if_else(is.na(state_descriptives$omega), 0, 1) # one-item measure don&#39;t have omega, so I make those see through ) + theme_cowplot() + scale_colour_manual(values=cb_palette) + scale_fill_manual(values = cb_palette) + theme( axis.text.y = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank(), strip.background.x = element_blank(), strip.background.y = element_blank(), legend.position = &quot;none&quot;, text = element_text(family = &quot;Corbel&quot;) ) -&gt; figure2 figure2 ggsave( here(&quot;figures&quot;, &quot;figure2.tiff&quot;), plot = figure2, width = 21 * 0.8, height = 29.7 * 0.4, units = &quot;cm&quot;, dpi = 300 ) Alright, last the smartphone use variables. dat &lt;- study1 %&gt;% select(all_of(c(&quot;id&quot;, &quot;social_media_objective&quot;, &quot;social_media_subjective&quot;, &quot;error&quot;))) %&gt;% # turn to hours mutate( across( contains(&quot;social_media&quot;), ~ .x /60 ) ) %&gt;% pivot_longer( -id, names_to = &quot;variable&quot;, values_to = &quot;value&quot; ) rename_levels &lt;- c( &quot;Objective (h)&quot;, &quot;Subjective (h)&quot;, &quot;Accuracy (%)&quot; ) my_string &lt;- &quot;_state&quot; dat &lt;- clean_plot_data(dat, rename_levels, my_string) social_media2 &lt;- clean_plot_data(social_media, rename_levels, my_string) %&gt;% filter(variable %in% c(&quot;Objective (h)&quot;, &quot;Subjective (h)&quot;, &quot;Accuracy (%)&quot;)) %&gt;% # add x axis position for geom_text mutate( x_position = case_when( variable == &quot;Accuracy (%)&quot; ~ 1200*0.8, TRUE ~ 0.8*13 ) ) And the plot. # function for breaks my_breaks &lt;- function(x) { if (max(x) &lt; 100){ seq(0, 13, 2) } else { c(-200, 0, 400, 800, 1200) } } # function for limits my_limits &lt;- function(x) { if (max(x) &lt; 100){ c(0, 13) } else { c(-200, 1200) } } # plot ggplot( dat, aes( x = value, y = 1, color = variable, fill = variable ) ) + geom_quasirandom(groupOnX=FALSE, size = 0.2, alpha = 0.5) + facet_wrap( ~ variable, scales = &quot;free_x&quot; ) + scale_x_continuous(breaks = my_breaks, limits = my_limits) + geom_text( data = social_media2, aes( x = x_position, y = 0.7, label = paste0(&quot;M = &quot;, mean), family = &quot;Corbel&quot; ), size = 3, color = &quot;black&quot; ) + geom_text( data = social_media2, aes( x = x_position, y = 0.67, label = paste0(&quot;SD = &quot;, sd), family = &quot;Corbel&quot; ), size = 3, color = &quot;black&quot; ) + theme_cowplot() + scale_colour_manual(values=cb_palette) + scale_fill_manual(values = cb_palette) + theme( axis.text.y = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank(), strip.background.x = element_blank(), strip.background.y = element_blank(), legend.position = &quot;none&quot;, text = element_text(family = &quot;Corbel&quot;) ) -&gt; figure3 figure3 ggsave( here(&quot;figures&quot;, &quot;figure3.tiff&quot;), plot = figure3, width = 21 * 0.8, height = 29.7 * 0.4, units = &quot;cm&quot;, dpi = 300 ) "],["analysis-study-1.html", "4 Analysis Study 1 4.1 Research questions 4.2 Data preparation 4.3 Do trait variables predict social media use and accuracy? 4.4 Do state (i.e., day-level) variables predict social media use and accuracy? 4.5 Does social media use predict well-being?", " 4 Analysis Study 1 I move on to the analyses. We have about a dozen models to run. All of these models should take into account that the data are nested (in participants and days). That means quite complex model structures, especially when we have many predictors. The lme4 package can handle such multi-level models, but in my experience suffers from many convergence issues that a Bayesian approach (i.e., the brms package) can handle better [paper on convergence comparison]. Moreover, corrections for multiple comparisons are less of a problem with Bayesian models. Last, although the models here are mostly exploratory, we can still incorporate prior knowledge from the literature (and common sense about our variables). 4.1 Research questions We have a total of 9 research questions. I divide them in three blocks: To what extent do specific person-level variables such as personality and motivational factors shape the accuracy of social media time engagement? To what extent do subjective experiences such as mood predict and or interact with person-level factors to shape the accuracy of social media time engagement? What are the unique relations relating objective and subjective engagement to well-being outcomes? Each of those questions comes with a number of models. Ill structure the analysis section according to those blocks. 4.2 Data preparation I processed the study1 data set in previous sections. Specifying priors on the untransformed variables can be tough. In some analyses (e.g., predicting social media use) Ill use centered predictors to make it easier to interpret the intercept. In other analyses (e.g., predicting error), I want to standardize outcomes and predictors of interest because standardized variables make it easier to choose sensible priors, especially because we can use standardized effect sizes reported in the literature. Standardizing also makes it easier to interpret the effect sizes. First, I transform all variables of interest (into new variables). study1 &lt;- study1 %&gt;% # center personality variables mutate( across( c( autonomy_state:relatedness_state, satisfied:enjoyable, autonomy_trait:openness ), ~ scale(.x, center = TRUE, scale = FALSE), .names = &quot;{col}_c&quot; # add &quot;_c&quot; suffix to new variables for &quot;centered&quot; ) ) Next, lets have a look of how many NAs there are per variable. Table 4.1 shows that missing values arent a big problem. The missings come from rows where participants didnt answer the diary, but did report screen time mostly. Table 4.1: Missing values per variable (column) name value social_media_subjective 51 pickups_subjective 38 notifications_subjective 38 social_media_objective 51 error 53 pickups_objective 30 weekly_notifications 0 well_being_state 40 autonomy_state 39 competence_state 39 relatedness_state 39 satisfied 42 boring 42 stressful 41 enjoyable 44 autonomy_trait 0 competence_trait 0 relatedness_trait 0 extraversion 0 agreeableness 0 conscientiousness 0 neuroticism 0 openness 0 4.3 Do trait variables predict social media use and accuracy? Our first research question asks whether personality traits and trait motivations predict the accuracy of social media time engagement. Specifically, we have three models that address the following sub-questions: Do person-level variables predict objective-only engagement? Do person-level variables predict subjective-only engagement? Do person-level variables predict accuracy? Ill construct a model for each of those three questions. Our dependent variables are clear-cut: social_media_objective, social_media_subjective, and error. Our predictors will be variables on the person-level, so trait/personality variables: the Big Five and the three self-determination motivations. 4.3.1 Model 1: Trait variables predicting objective use First, I choose sensible priors. Theres some literature out there on Big Five and smartphone use, as well as motivations and smartphone use. If we only knew the mean and variance of the social media estimates, a Gaussian distribution would be most appropriate. However, we do know more than just those two parameters. Namely, we know that the scale is continuous (i.e., time) and cannot be less than zero. Also, if we look at the distribution of time on an activity, the variance usually increases with the mean. Therefore, a gamma distribution appears more adequate to me. That means the models will use a log-link, which makes it hard to have an intuition about prior distributions (at least for me). Thus, I follow the recommendations of McElreath and simulate the priors. For the intercept, we can look at previous research. For example, this paper has average phone use times of about two hours per participant per day. Here, we only looked at social media use, so as a guess-timate with reasonable uncertainty, Ill choose a lognormal distribution with a meanlog of 4.5 and a meansd of 0.8. See the upper left panel of Figure 4.1. That intercept has mots its value below two hours, but allows substantial skew for a couple of heavy users. For the shape of the gamma distribution, I also played around and settled on one that somewhat resembles our assumptions on the intercept, such that the majority of values will be below five hours with a couple of heavy users (see left side upper panel). For the Big Five fixed effects, this paper predicted social media use with the Big Five over two time period in a longitudinal design. They found that only neuroticism was related to social media use, but that effect was extremely small (\\(\\beta\\) = .028). None of the other effects were large. Therefore, Ill use rather flat, weakly regularizing priors for those effects. In 4.1, lower panel, Ill use the prior on the left, because its skeptical, but regularizing. The one on the right might be too optimistic by suggesting a small positive effect. For basic psychological needs, theres quite some literature on these needs and pathological smartphone use (smartphone addiction). However, theres little info we could use for priors, so Ill just go with the same prior as for the Big Five. See the top right of the Figure below. For all other parameters, Ill take the brms default priors. Credit for the code for the figures goes to Solomon Kurz. Figure 4.1: Prior simmulations for Model 1 Lets set those priors we simulated above. priors_model1 &lt;- c( # intercept prior(normal(4.5, 0.8), class = Intercept), # prior on effects prior(normal(0, 0.1), class = b), # all other effects prior(gamma(2.5, 100), class = shape) ) Alright, time to run the model. Luckily, none of these variables have missing values, so I wont need to model missings in this model. Note that I ran the block below once and stored the model. Those fit objects are too large for Github, so you can download them here [give Onedrive link]. model1 &lt;- brm( data = study1, family = Gamma(link = &quot;log&quot;), prior = priors_model1, social_media_objective ~ 1 + openness_c + conscientiousness_c + extraversion_c + agreeableness_c + neuroticism_c + competence_trait_c + relatedness_trait_c + autonomy_trait_c + (1 | id) + (1 | day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = 0.99), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model1_study1&quot;) ) Lets inspect the traceplots. Overall, they look fine and the chains seem to have mixed well, see (Figure ??). The residual standard deviation around the day grouping doesnt look ideal though. Variation between days wasnt that great, so the model estimates quite a lot variances that are close to zero. We could remove the day grouping, but I think we have theoretical reasons to keep it, namely to account for all known sources of variation. Figure 4.2: Traceplots and posterior distributions for Model 1 Figure 4.3: Traceplots and posterior distributions for Model 1 Figure 4.4: Traceplots and posterior distributions for Model 1 The posterior predictive distribution (Figure 4.5) shows that the model does an okay job in predicting our outcome, with little indication of overfitting. If anything, it looks like the model somewhat underestimates the social media time of users. Then again, the LOO-PIT graphs show that the model could be better, possibly because it doesnt do the best job in predicting the majority of cases (i.e., the underestimate). Figure 4.5: Posterior predictive checks for Model 1 Lets also check for potentially influential values. None are tagged as influential, which increases my trust in the model. loo(model1) ## ## Computed from 12000 by 428 log-likelihood matrix ## ## Estimate SE ## elpd_loo -2440.0 14.4 ## p_loo 20.7 1.4 ## looic 4880.0 28.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Alright, time to look at the summary: neuroticism and competence are the only predictors whose posterior distribution isnt centered on zero. That said, we cannot be 95% certain the true value doesnt contain zero. summary(model1, priors = TRUE) ## Family: gamma ## Links: mu = log; shape = identity ## Formula: social_media_objective ~ 1 + openness_c + conscientiousness_c + extraversion_c + agreeableness_c + neuroticism_c + competence_trait_c + relatedness_trait_c + autonomy_trait_c + (1 | id) + (1 | day) ## Data: study1 (Number of observations: 428) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b ~ normal(0, 0.1) ## Intercept ~ normal(4.5, 0.8) ## sd ~ student_t(3, 0, 2.5) ## shape ~ gamma(2.5, 100) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.06 0.07 0.00 0.23 1.00 5873 6154 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.49 0.07 0.35 0.64 1.00 3739 6583 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 4.82 0.08 4.67 4.97 1.00 5813 6625 ## openness_c 0.00 0.08 -0.16 0.16 1.00 11686 9877 ## conscientiousness_c -0.01 0.08 -0.16 0.15 1.00 12552 9686 ## extraversion_c 0.10 0.07 -0.04 0.24 1.00 9902 9429 ## agreeableness_c -0.07 0.08 -0.22 0.09 1.00 10920 9571 ## neuroticism_c 0.12 0.08 -0.05 0.28 1.00 11221 9205 ## competence_trait_c -0.10 0.07 -0.24 0.05 1.00 8796 7816 ## relatedness_trait_c -0.01 0.06 -0.13 0.11 1.00 9211 9015 ## autonomy_trait_c -0.04 0.07 -0.17 0.09 1.00 8964 8810 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 1.47 0.10 1.27 1.68 1.00 9250 8357 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.6) shows that visually, but also shows that the model does a fine job in describing the data, given the relatively (at least on the log scale) narrow 95% credible interval. Overall, theres little evidence that personality traits are related so smartphone use, except for some tentative evidence that neurotic people use more social media and those who generally feel a level of competence in their lives use them less. Figure 4.6: Effects plot for Model 1 4.3.2 Model 2: Trait variables predicting subjective use Next up, I predict subjective use from the same predictors. We know that objective and subjective use arent perfectly correlated. Then again, the priors for Model 1 were only weakly regularizing, which is why I use the same priors again. priors_model2 &lt;- c( # intercept prior(normal(4.5, 0.8), class = Intercept), # prior on effects prior(normal(0, 0.1), class = b), # all other effects prior(gamma(2.5, 100), class = shape) ) Alright, time to run the model. The subjective social media estimate had one missing value. Usually Id impute missing values during model fitting, but with one, I think its safe to drop it. model2 &lt;- brm( data = study1, family = Gamma(link = &quot;log&quot;), prior = priors_model2, social_media_subjective ~ 1 + openness_c + conscientiousness_c + extraversion_c + agreeableness_c + neuroticism_c + competence_trait_c + relatedness_trait_c + autonomy_trait_c + (1 | id) + (1 | day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = 0.999), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model2_study1&quot;) ) Lets inspect the traceplots. Overall, they look fine and the chains seem to have mixed well (Figure ??). Again, everything looks fine. Figure 4.7: Traceplots and posterior distributions for Model 2 Figure 4.8: Traceplots and posterior distributions for Model 2 Figure 4.9: Traceplots and posterior distributions for Model 2 The posterior predictive distribution (Figure ??) looks similar to Model 1, with a tendency of the model to underestimate self-reported social media use. All other diagnostics look similar, which is not surprising given that the outcomes of Model 1 and Model 2 are correlated. Overall, the model fit is okay-ish, but certainly not excellent. Figure 4.10: Posterior predictive checks for Model 2 Lets again check for potentially influential values. The model diagnostics look good. Even though there were several rather large raw values on the outcome variable, the model expects them because we model the outcome as a Gamma distribution. We need to calculate ELPD directly, which shows no outliers. loo(model2, reloo = TRUE) ## 1 problematic observation(s) found. ## The model will be refit 1 times. ## ## Fitting model 1 out of 1 (leaving out observation 392) ## Start sampling ## ## Computed from 12000 by 428 log-likelihood matrix ## ## Estimate SE ## elpd_loo -2502.2 15.9 ## p_loo 24.6 1.6 ## looic 5004.4 31.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 421 98.4% 412 ## (0.5, 0.7] (ok) 7 1.6% 2552 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Alright, time to look at the summary. This time, all posterior distributions are mostly centered around zero, so we can be 95% (always conditional on the model) certain that personality traits and motivations are not meaningfully (i.e., large effect) related to self-reported social media use. Maybe autonomy gets close, but the posterior distributions still includes zero and small negative effects. summary(model2, priors = TRUE) ## Family: gamma ## Links: mu = log; shape = identity ## Formula: social_media_subjective ~ 1 + openness_c + conscientiousness_c + extraversion_c + agreeableness_c + neuroticism_c + competence_trait_c + relatedness_trait_c + autonomy_trait_c + (1 | id) + (1 | day) ## Data: study1 (Number of observations: 428) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b ~ normal(0, 0.1) ## Intercept ~ normal(4.5, 0.8) ## sd ~ student_t(3, 0, 2.5) ## shape ~ gamma(2.5, 100) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.07 0.07 0.00 0.25 1.00 5313 5290 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.51 0.08 0.36 0.66 1.00 3414 5977 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 4.92 0.08 4.76 5.08 1.00 5160 5656 ## openness_c -0.04 0.08 -0.21 0.12 1.00 9524 8983 ## conscientiousness_c -0.08 0.08 -0.24 0.08 1.00 8766 9443 ## extraversion_c 0.06 0.07 -0.08 0.20 1.00 7976 7997 ## agreeableness_c -0.04 0.08 -0.19 0.12 1.00 8628 8552 ## neuroticism_c 0.06 0.08 -0.10 0.21 1.00 9739 9263 ## competence_trait_c -0.04 0.07 -0.19 0.10 1.00 8340 8625 ## relatedness_trait_c -0.02 0.06 -0.14 0.10 1.00 7666 7707 ## autonomy_trait_c -0.09 0.07 -0.22 0.04 1.00 7668 8591 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 1.33 0.09 1.16 1.52 1.00 6185 9161 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.11) visually confirms that none of the predictors seem influential. Figure 4.11: Effects plot for Model 2 4.3.3 Model 3: Trait variables predicting accuracy For this model, its hard to choose informed priors, mainly because theres no literature on the relation between personality traits and accuracy. Also, its unclear what distribution for accuracy we should expect prior to having seen the data. The few papers out there show that people generally overestimate their (social) media use, which would speak for centering the distribution on a positive value (i.e., positive error). Other than that, Id expect a normal distribution, quite likely with fat tails, and possibly right skewed. Therefore, Ill use a student-t distribution outcome family. I start with a t distribution as prior for the intercept that is slightly centered on overestimates (i.e., 20%), but with quite some potential for extreme values and fairly fat tails. See Figure 4.12 left panel, for a visualization. As for the effects of the personality traits, Ill be skeptical of any effects and thus use weakly regularizing normal priors that are centered on zero, with a small range for the effect: If a person goes from average on a trait to one above average, wed expect that 95% of the effects should be between -50% and +50% (i.e., SD of 25). See the right panel in the figure below for a visualization. Figure 4.12: Prior simmulations for Model 2 Then lets set those priors we simulated above. priors_model3 &lt;- c( # intercept prior(student_t(10, 20, 100), class = Intercept), # all other effects prior(normal(0, 25), class = b) ) error had 53 missing values. Again, with such few cases, Im fine with dropping those during model fitting. model3 &lt;- brm( data = study1, family = student, prior = priors_model3, error ~ 1 + openness_c + conscientiousness_c + extraversion_c + agreeableness_c + neuroticism_c + competence_trait_c + relatedness_trait_c + autonomy_trait_c + (1 | id) + (1 | day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = 0.99), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model3_study1&quot;) ) Overall, the traceplots look fine and the chains seem to have mixed well, see (Figure ??). Again, the variance around the residuals for the day grouping is estimated to be small and zero often. Figure 4.13: Traceplots and posterior distributions for Model 3 Figure 4.14: Traceplots and posterior distributions for Model 3 Figure 4.15: Traceplots and posterior distributions for Model 3 The posterior predictive checks (Figure ??) look good. Note that the upper left panel looks strange because of the massive scale on the x-axis, which is why I reproduce it again at the bottom. We see that the student t distribution assigns too much posterior mass to negative values, whereas the data are right skewed. For even better fit, we might think about fitting a skew-normal outcome distribution, but Id say that the student-t was most appropriate before seeing the data. We see that the current models does an okay job in recovering the mean and the median. Figure 4.16: Posterior predictive checks for Model 3 Figure 4.17: Posterior predictive checks for Model 3 Lets again check for potentially influential values. Three potentially influential cases are flagged, which is why we calculate them precisely by setting reloo to TRUE. When we calculate ELPD directly, all values appear unproblematic. The results seem trustworthy. loo(model3, reloo = TRUE) ## 3 problematic observation(s) found. ## The model will be refit 3 times. ## ## Fitting model 1 out of 3 (leaving out observation 40) ## ## Fitting model 2 out of 3 (leaving out observation 43) ## ## Fitting model 3 out of 3 (leaving out observation 111) ## Start sampling ## Start sampling ## Start sampling ## ## Computed from 12000 by 426 log-likelihood matrix ## ## Estimate SE ## elpd_loo -2415.2 31.4 ## p_loo 136.1 6.7 ## looic 4830.3 62.7 ## ------ ## Monte Carlo SE of elpd_loo is 0.2. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 421 98.8% 436 ## (0.5, 0.7] (ok) 5 1.2% 714 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Lets inspect the summary. There doesnt seem to be much going on when it comes to the predictors. For all predictors, we cannot be 95% certain that zero isnt the true effect (conditional on the model). Again, neuroticism comes close. summary(model3, priors = TRUE) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: error ~ 1 + openness_c + conscientiousness_c + extraversion_c + agreeableness_c + neuroticism_c + competence_trait_c + relatedness_trait_c + autonomy_trait_c + (1 | id) + (1 | day) ## Data: study1 (Number of observations: 426) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b ~ normal(0, 25) ## Intercept ~ student_t(10, 20, 100) ## nu ~ gamma(2, 0.1) ## sd ~ student_t(3, 0, 57.7) ## sigma ~ student_t(3, 0, 57.7) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 3.21 3.59 0.11 11.79 1.00 5317 5339 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 64.47 6.40 52.80 77.92 1.00 2251 3791 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 13.34 7.62 -1.10 28.66 1.00 2110 4056 ## openness_c -16.81 13.17 -42.62 9.80 1.00 2867 4647 ## conscientiousness_c -10.92 13.30 -36.68 15.14 1.00 2861 5033 ## extraversion_c -3.45 9.78 -22.74 16.05 1.00 2531 4145 ## agreeableness_c -8.85 12.86 -34.32 16.69 1.00 2758 4088 ## neuroticism_c -21.89 13.39 -48.27 4.58 1.00 2828 4561 ## competence_trait_c 11.31 10.95 -10.34 32.57 1.00 2710 4601 ## relatedness_trait_c -0.79 9.09 -18.85 16.70 1.00 2056 3941 ## autonomy_trait_c -2.66 9.59 -21.25 16.33 1.00 2594 4595 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 23.26 2.23 19.16 27.85 1.00 5374 7711 ## nu 1.20 0.11 1.02 1.46 1.00 5581 4487 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.18) visually confirms that none of the predictors seem largely influential. If anything, it appears neurotic people might underestimate their social media use, but again their interval contains zero. Figure 4.18: Effects plot for Model 3 4.4 Do state (i.e., day-level) variables predict social media use and accuracy? For the next section, we look at whether day-level variables (i.e., variables reported during experience sampling) predict subjective use, objective use, and accuracy. Again, we have three questions: Do day-level variables predict objective-only engagement? Do day-level variables predict subjective-only engagement? Do day-level variables predict accuracy? Once more Ill construct a model for each of those three questions. Our dependent variables are the same as in the previous block of models: social_media_objective, social_media_subjective, and error. Our predictors will be variables on the day-level, so state variables: need satisfaction for autonomy, competence, and relatedness as well as four experiential qualities during participants days (boredom, enjoyment, satisfaction, and stress). Because we want to separate between-person and within-person effects, well do group-mean centering (i.e., per participant) and the calculate the deviation of each observation from that group mean. We enter both the person mean and their deviation as predictors, which will lead to a lot of variables, because each of the seven predictors will be separated into a between-person and a within-person predictor. See this blogpost for a tutorial on centering. study1 &lt;- study1 %&gt;% group_by(id) %&gt;% mutate( across( c(autonomy_state:relatedness_state, satisfied:enjoyable), list( between = ~ mean(.x, na.rm = TRUE), within = ~.x - mean(.x, na.rm = TRUE) ) ) ) %&gt;% ungroup() 4.4.1 Model 4: State variables predicting objective use First, I choose sensible priors. In contrast to previous models, there isnt much literature that could inform priors on our predictors. The same goes for the experiential qualities. Well go with a maximal model where each participant and day get their own intercept plus random slopes nested within participant, because its plausible that the effects vary per participant. They could also vary by day, but a) there was little to no variation for day in previous models, and b) that would lead to more parameters than our little data could handle. Just like before we assume a Gamma distribution for the social media variables. Ill go step-by-step: For the intercepts, I use the same prior as above in Models 1 and 2. For basic psychological needs fixed effects, this paper reports very small relations between self-determined motivations at work and social media use (\\(\\beta\\) &lt; .06). However, most of the literature focuses on those motivations and social media addiction, enjoyment of social media, or satisfaction with social media. Therefore, Ill use weakly regularizing priors for those effects, which means Ill go with the same prior for the effects as for Models 1 and 2. I would expect larger differences on the between-level, based on the literature on media use and well-being. However, we dont have that info for our predictors and the priors are rather weak, so Ill apply them to both between and within predictors. For the experiential qualities, there isnt much literature out there that would allow choosing an informed prior. Most of those experiential qualities are about having a fulfilled and good day, which taps into the whole controversy over the relation between such experiences and social media. Therefore, Ill take a skeptical stance here and again assign the same weakly regularizing priors (aka the priors we also used for Models 1 and 2). For the variances (sigmas) Ill take the brms default priors because I have no prior information, nor how the effects vary across day and participant. For the correlation between intercepts and slopes, Ill again use the default prior, mostly because those have shown to help with convergence and I dont have good information on which correlation to expect. priors_model4 &lt;- c( # intercept prior(normal(4.5, 0.8), class = Intercept), # prior on effects prior(normal(0, 0.1), class = b), # prior on shape prior(gamma(2.5, 100), class = shape) ) Lets run the model. Note that the within-deviations can vary per participant, but not the between effects. model4 &lt;- brm( data = study1, family = Gamma(link = &quot;log&quot;), prior = priors_model4, social_media_objective ~ 1 + autonomy_state_between + competence_state_between + relatedness_state_between + satisfied_between + boring_between + stressful_between + enjoyable_between + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within + ( 1 + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within | id ) + (1 |day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list( adapt_delta = 0.99 ), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model4_study1&quot;) ) Overall, the traceplots look fine and the chains seem to have mixed well (Figure ??). Again the variance around the day grouping is small and mostly estimated to be zero or close to zero. Figure 4.19: Traceplots and posterior distributions for Model 4 Figure 4.20: Traceplots and posterior distributions for Model 4 Figure 4.21: Traceplots and posterior distributions for Model 4 Figure 4.22: Traceplots and posterior distributions for Model 4 Figure 4.23: Traceplots and posterior distributions for Model 4 Figure 4.24: Traceplots and posterior distributions for Model 4 Figure 4.25: Traceplots and posterior distributions for Model 4 Figure 4.26: Traceplots and posterior distributions for Model 4 Figure 4.27: Traceplots and posterior distributions for Model 4 Figure 4.28: Traceplots and posterior distributions for Model 4 Figure 4.29: Traceplots and posterior distributions for Model 4 The posterior predictive check shows that the model does a mediocre job. The model does a good job retrieving the mean, but underestimates social media use once more. Also, the LOO-PIT diagnostics look like a different distribution will fit better, potentially a skewed normal or beta-distribution. Figure 4.30: Posterior predictive checks for Model 4 Next, we check for potentially influential cases. There are none. loo(model4) ## ## Computed from 12000 by 420 log-likelihood matrix ## ## Estimate SE ## elpd_loo -2412.8 14.1 ## p_loo 25.6 1.6 ## looic 4825.6 28.1 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 408 97.1% 4830 ## (0.5, 0.7] (ok) 12 2.9% 1078 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Lets inspect the summary. There doesnt seem to be much going on when it comes to the predictors. summary(model4, priors = TRUE) ## Family: gamma ## Links: mu = log; shape = identity ## Formula: social_media_objective ~ 1 + autonomy_state_between + competence_state_between + relatedness_state_between + satisfied_between + boring_between + stressful_between + enjoyable_between + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within + (1 + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within | id) + (1 | day) ## Data: study1 (Number of observations: 420) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b ~ normal(0, 0.1) ## Intercept ~ normal(4.5, 0.8) ## L ~ lkj_corr_cholesky(1) ## sd ~ student_t(3, 0, 2.5) ## shape ~ gamma(2.5, 100) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.07 0.07 0.00 0.25 1.00 5832 6519 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.52 0.08 0.38 0.67 1.00 3573 5944 ## sd(autonomy_state_within) 0.05 0.04 0.00 0.16 1.00 8320 5372 ## sd(competence_state_within) 0.05 0.04 0.00 0.14 1.00 8935 5381 ## sd(relatedness_state_within) 0.07 0.05 0.00 0.19 1.00 7787 4916 ## sd(satisfied_within) 0.04 0.03 0.00 0.12 1.00 8933 5890 ## sd(boring_within) 0.03 0.03 0.00 0.10 1.00 9702 5442 ## sd(stressful_within) 0.03 0.02 0.00 0.09 1.00 9082 6157 ## sd(enjoyable_within) 0.04 0.03 0.00 0.12 1.00 9115 5934 ## cor(Intercept,autonomy_state_within) -0.02 0.33 -0.65 0.61 1.00 16387 8961 ## cor(Intercept,competence_state_within) -0.01 0.34 -0.64 0.63 1.00 21044 9001 ## cor(autonomy_state_within,competence_state_within) -0.04 0.34 -0.66 0.60 1.00 13676 9802 ## cor(Intercept,relatedness_state_within) 0.00 0.33 -0.62 0.63 1.00 20886 8610 ## cor(autonomy_state_within,relatedness_state_within) -0.02 0.33 -0.65 0.61 1.00 14368 8242 ## cor(competence_state_within,relatedness_state_within) -0.02 0.33 -0.65 0.62 1.00 11400 8640 ## cor(Intercept,satisfied_within) -0.02 0.33 -0.65 0.62 1.00 19647 8386 ## cor(autonomy_state_within,satisfied_within) -0.03 0.33 -0.65 0.62 1.00 13873 8112 ## cor(competence_state_within,satisfied_within) -0.03 0.34 -0.66 0.60 1.00 11687 9023 ## cor(relatedness_state_within,satisfied_within) -0.03 0.33 -0.65 0.62 1.00 10544 9670 ## cor(Intercept,boring_within) 0.05 0.33 -0.60 0.67 1.00 21420 8481 ## cor(autonomy_state_within,boring_within) 0.02 0.34 -0.62 0.64 1.00 16012 9390 ## cor(competence_state_within,boring_within) 0.02 0.33 -0.61 0.64 1.00 12124 9240 ## cor(relatedness_state_within,boring_within) 0.01 0.33 -0.62 0.63 1.00 9880 9056 ## cor(satisfied_within,boring_within) 0.01 0.34 -0.63 0.64 1.00 9165 9884 ## cor(Intercept,stressful_within) 0.01 0.33 -0.63 0.64 1.00 18727 8771 ## cor(autonomy_state_within,stressful_within) 0.04 0.34 -0.61 0.66 1.00 14611 8703 ## cor(competence_state_within,stressful_within) 0.03 0.33 -0.62 0.65 1.00 11626 9342 ## cor(relatedness_state_within,stressful_within) 0.01 0.33 -0.62 0.65 1.00 10591 9391 ## cor(satisfied_within,stressful_within) 0.01 0.33 -0.62 0.64 1.00 8292 9818 ## cor(boring_within,stressful_within) -0.02 0.33 -0.64 0.62 1.00 8189 9968 ## cor(Intercept,enjoyable_within) -0.04 0.33 -0.66 0.62 1.00 19274 7944 ## cor(autonomy_state_within,enjoyable_within) -0.04 0.34 -0.68 0.62 1.00 13860 8987 ## cor(competence_state_within,enjoyable_within) -0.03 0.34 -0.66 0.61 1.00 11180 8545 ## cor(relatedness_state_within,enjoyable_within) -0.02 0.34 -0.65 0.63 1.00 9846 9047 ## cor(satisfied_within,enjoyable_within) -0.05 0.33 -0.66 0.61 1.00 8402 9749 ## cor(boring_within,enjoyable_within) 0.03 0.33 -0.61 0.65 1.00 7006 9546 ## cor(stressful_within,enjoyable_within) 0.02 0.33 -0.63 0.64 1.00 7054 9355 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 4.61 0.66 3.32 5.89 1.00 7471 8345 ## autonomy_state_between -0.02 0.08 -0.17 0.12 1.00 9979 9622 ## competence_state_between 0.01 0.07 -0.13 0.15 1.00 9383 10215 ## relatedness_state_between -0.01 0.07 -0.15 0.14 1.00 9084 9159 ## satisfied_between -0.04 0.07 -0.18 0.11 1.00 10538 9759 ## boring_between 0.05 0.06 -0.07 0.16 1.00 7557 9219 ## stressful_between 0.01 0.05 -0.10 0.11 1.00 7654 9029 ## enjoyable_between 0.07 0.07 -0.07 0.22 1.00 9402 8694 ## autonomy_state_within -0.01 0.06 -0.12 0.10 1.00 16434 8971 ## competence_state_within -0.06 0.06 -0.17 0.05 1.00 15497 9011 ## relatedness_state_within 0.01 0.06 -0.11 0.13 1.00 19434 9421 ## satisfied_within 0.02 0.05 -0.08 0.11 1.00 15316 8718 ## boring_within -0.01 0.04 -0.09 0.06 1.00 15776 9182 ## stressful_within -0.02 0.04 -0.09 0.05 1.00 16505 9577 ## enjoyable_within 0.03 0.05 -0.06 0.12 1.00 14243 9107 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 1.41 0.10 1.22 1.62 1.00 8991 8328 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.31) visually confirms that none of the predictors seem influential. Note that the correlations between random effects are on different scales which explains why the CI around them are so wide. Except for the correlations, all effects are on the log scale because we used a Gamma distribution, but the correlations are regularly on a scale of -1 to 1. Figure 4.31: Effects plot for Model 4 4.4.2 Model 5: State variables predicting subjective use The next model predicts subjective use from the same predictors. Once more, we know that subjective and objective use arent perfectly correlated, but the priors for Model 4 again are our best bet here (for both within-person and between-person effects). priors_model5 &lt;- c( # intercept prior(normal(4.5, 0.8), class = Intercept), # prior on effects prior(normal(0, 0.1), class = b), # prior on shape prior(gamma(2.5, 100), class = shape) ) Lets run the model. Like with Model 2, we dont separately model zeros. model5 &lt;- brm( data = study1, family = Gamma(link = &quot;log&quot;), prior = priors_model4, social_media_subjective ~ 1 + autonomy_state_between + competence_state_between + relatedness_state_between + satisfied_between + boring_between + stressful_between + enjoyable_between + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within + ( 1 + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within | id ) + (1 |day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list( adapt_delta = 0.99 ), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model5_study1&quot;) ) Overall, the traceplots look fine and the chains seem to have mixed well (Figure ??). Like before, the variance around the day grouping is small and mostly estimated to be zero or close to zero. Figure 4.32: Traceplots and posterior distributions for Model 4 Figure 4.33: Traceplots and posterior distributions for Model 4 Figure 4.34: Traceplots and posterior distributions for Model 4 Figure 4.35: Traceplots and posterior distributions for Model 4 Figure 4.36: Traceplots and posterior distributions for Model 4 Figure 4.37: Traceplots and posterior distributions for Model 4 Figure 4.38: Traceplots and posterior distributions for Model 4 Figure 4.39: Traceplots and posterior distributions for Model 4 Figure 4.40: Traceplots and posterior distributions for Model 4 Figure 4.41: Traceplots and posterior distributions for Model 4 Figure 4.42: Traceplots and posterior distributions for Model 4 The posterior predictive check shows about the same model fit as Model 4: mediocre. It overestimates the frequency of low values compared to the raw data. I could change the prior, but I also dont want to overfit just based on the raw data. Figure 4.43: Posterior predictive checks for Model 5 There are no cases flagged as potentially influential outliers. loo(model5, reloo = TRUE) ## 1 problematic observation(s) found. ## The model will be refit 1 times. ## ## Fitting model 1 out of 1 (leaving out observation 384) ## Start sampling ## ## Computed from 12000 by 420 log-likelihood matrix ## ## Estimate SE ## elpd_loo -2466.6 15.8 ## p_loo 28.8 1.6 ## looic 4933.2 31.6 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 393 93.6% 1996 ## (0.5, 0.7] (ok) 27 6.4% 1886 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Lets inspect the summary. Having an enjoyable and boring day might be related to estimating more social media use (on the between level). Conversely, a feeling of competence might decrease estimates. However, all three of those posterior distributions include zero, even though its close. summary(model5, priors = TRUE) ## Family: gamma ## Links: mu = log; shape = identity ## Formula: social_media_subjective ~ 1 + autonomy_state_between + competence_state_between + relatedness_state_between + satisfied_between + boring_between + stressful_between + enjoyable_between + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within + (1 + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within | id) + (1 | day) ## Data: study1 (Number of observations: 420) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b ~ normal(0, 0.1) ## Intercept ~ normal(4.5, 0.8) ## L ~ lkj_corr_cholesky(1) ## sd ~ student_t(3, 0, 2.5) ## shape ~ gamma(2.5, 100) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.07 0.08 0.00 0.26 1.00 6115 6209 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.49 0.08 0.35 0.64 1.00 4609 6905 ## sd(autonomy_state_within) 0.06 0.04 0.00 0.16 1.00 9124 6247 ## sd(competence_state_within) 0.05 0.04 0.00 0.16 1.00 9417 5712 ## sd(relatedness_state_within) 0.07 0.05 0.00 0.20 1.00 10131 6334 ## sd(satisfied_within) 0.05 0.04 0.00 0.14 1.00 8222 5408 ## sd(boring_within) 0.04 0.03 0.00 0.11 1.00 9518 5873 ## sd(stressful_within) 0.04 0.03 0.00 0.10 1.00 9568 6523 ## sd(enjoyable_within) 0.04 0.03 0.00 0.13 1.00 8525 6408 ## cor(Intercept,autonomy_state_within) -0.02 0.34 -0.65 0.63 1.00 26697 8853 ## cor(Intercept,competence_state_within) -0.03 0.33 -0.65 0.61 1.00 25740 8665 ## cor(autonomy_state_within,competence_state_within) -0.04 0.34 -0.65 0.61 1.00 17150 9376 ## cor(Intercept,relatedness_state_within) 0.01 0.33 -0.63 0.65 1.00 27531 8584 ## cor(autonomy_state_within,relatedness_state_within) -0.02 0.34 -0.66 0.62 1.00 16808 9191 ## cor(competence_state_within,relatedness_state_within) -0.02 0.33 -0.64 0.61 1.00 12371 9241 ## cor(Intercept,satisfied_within) 0.01 0.33 -0.62 0.64 1.00 23477 8466 ## cor(autonomy_state_within,satisfied_within) -0.03 0.33 -0.66 0.62 1.00 16534 8899 ## cor(competence_state_within,satisfied_within) -0.03 0.34 -0.67 0.62 1.00 12832 9212 ## cor(relatedness_state_within,satisfied_within) -0.03 0.33 -0.65 0.61 1.00 10261 10017 ## cor(Intercept,boring_within) 0.04 0.33 -0.60 0.65 1.00 21811 8984 ## cor(autonomy_state_within,boring_within) 0.03 0.33 -0.61 0.66 1.00 17402 9064 ## cor(competence_state_within,boring_within) 0.02 0.33 -0.62 0.65 1.00 13863 8855 ## cor(relatedness_state_within,boring_within) 0.01 0.34 -0.63 0.64 1.00 10166 9533 ## cor(satisfied_within,boring_within) 0.02 0.33 -0.61 0.64 1.00 9338 9412 ## cor(Intercept,stressful_within) 0.04 0.33 -0.59 0.64 1.00 25952 7970 ## cor(autonomy_state_within,stressful_within) 0.03 0.33 -0.62 0.66 1.00 15050 8199 ## cor(competence_state_within,stressful_within) 0.03 0.34 -0.62 0.66 1.00 12065 8725 ## cor(relatedness_state_within,stressful_within) 0.01 0.33 -0.62 0.64 1.00 9962 9381 ## cor(satisfied_within,stressful_within) 0.01 0.33 -0.62 0.63 1.00 9639 9836 ## cor(boring_within,stressful_within) -0.01 0.33 -0.64 0.62 1.00 7948 9156 ## cor(Intercept,enjoyable_within) -0.05 0.33 -0.65 0.59 1.00 24227 9637 ## cor(autonomy_state_within,enjoyable_within) -0.03 0.34 -0.66 0.61 1.00 15460 8292 ## cor(competence_state_within,enjoyable_within) -0.03 0.33 -0.65 0.60 1.00 12580 8950 ## cor(relatedness_state_within,enjoyable_within) -0.02 0.33 -0.64 0.61 1.00 11135 9699 ## cor(satisfied_within,enjoyable_within) -0.04 0.34 -0.67 0.60 1.00 8582 9049 ## cor(boring_within,enjoyable_within) 0.03 0.33 -0.62 0.65 1.00 8034 9232 ## cor(stressful_within,enjoyable_within) 0.02 0.34 -0.63 0.66 1.00 6775 10061 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 5.36 0.66 4.06 6.63 1.00 11737 10046 ## autonomy_state_between -0.07 0.08 -0.22 0.08 1.00 13941 8408 ## competence_state_between -0.09 0.07 -0.23 0.05 1.00 12371 9819 ## relatedness_state_between 0.01 0.07 -0.14 0.15 1.00 14157 8970 ## satisfied_between -0.10 0.08 -0.25 0.05 1.00 13708 8537 ## boring_between 0.09 0.06 -0.02 0.21 1.00 11432 9251 ## stressful_between -0.02 0.05 -0.12 0.08 1.00 10961 9963 ## enjoyable_between 0.12 0.08 -0.03 0.26 1.00 13528 9775 ## autonomy_state_within -0.00 0.06 -0.12 0.11 1.00 23098 9607 ## competence_state_within -0.04 0.06 -0.15 0.07 1.00 19223 8865 ## relatedness_state_within -0.00 0.06 -0.13 0.12 1.00 23114 9482 ## satisfied_within -0.03 0.05 -0.12 0.07 1.00 18050 9322 ## boring_within 0.02 0.04 -0.06 0.10 1.00 21043 9152 ## stressful_within -0.02 0.04 -0.10 0.05 1.00 22686 9812 ## enjoyable_within 0.04 0.05 -0.06 0.14 1.00 19642 8730 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 1.30 0.09 1.12 1.49 1.00 11230 9042 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.44) visually that those three states might be influential, but their CIs overlap with zero. Figure 4.44: Effects plot for Model 5 4.4.3 Model 6: State variables predicting accuracy For this model, once more its hard to choose informed priors. Therefore, I choose the same skeptical, but weakly regularizing priors as for Model 3, with a student t distribution with fat tails, an intercept that reflects a tendency to overestimate, and slopes that are skeptical of large effects. For everything else, Ill use the brms default priors because I dont have information on what correlation or variances to expect. Note that I use the priors on the slope for both between and within effects, simply because I dont have an informed guess about the two being different. priors_model6 &lt;- c( # intercept prior(student_t(10, 20, 100), class = Intercept), # all other effects prior(normal(0, 25), class = b) ) Lets run the model. model6 &lt;- brm( data = study1, family = student, prior = priors_model6, error ~ 1 + autonomy_state_between + competence_state_between + relatedness_state_between + satisfied_between + boring_between + stressful_between + enjoyable_between + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within + ( 1 + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within | id ) + (1 |day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list( adapt_delta = 0.99 ), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model6_study1&quot;) ) Overall, the traceplots look fine and the chains seem to have mixed well, see (Figure ??), just like with Model 3. Figure 4.45: Traceplots and posterior distributions for Model 6 Figure 4.46: Traceplots and posterior distributions for Model 6 Figure 4.47: Traceplots and posterior distributions for Model 6 Figure 4.48: Traceplots and posterior distributions for Model 6 Figure 4.49: Traceplots and posterior distributions for Model 6 Figure 4.50: Traceplots and posterior distributions for Model 6 Figure 4.51: Traceplots and posterior distributions for Model 6 Figure 4.52: Traceplots and posterior distributions for Model 6 Figure 4.53: Traceplots and posterior distributions for Model 6 Figure 4.54: Traceplots and posterior distributions for Model 6 Figure 4.55: Traceplots and posterior distributions for Model 6 The posterior predictive checks (Figure ??) look good. Like with Model 3, that the upper left panel looks strange because of the massive scale on the x-axis. I reproduce it again at the bottom with a more sensible x-axis. Once more, the student t distribution assigns too much posterior mass to negative values, whereas the data are right skewed. Otherwise, the model does an okay job in recovering the mean and the median. Figure 4.56: Posterior predictive checks for Model 6 Figure 4.57: Posterior predictive checks for Model 6 Two cases are flagged as potential outliers, which disappear once calculated directly. loo(model6, reloo = TRUE) ## 2 problematic observation(s) found. ## The model will be refit 2 times. ## ## Fitting model 1 out of 2 (leaving out observation 39) ## ## Fitting model 2 out of 2 (leaving out observation 128) ## Start sampling ## Start sampling ## ## Computed from 12000 by 418 log-likelihood matrix ## ## Estimate SE ## elpd_loo -2368.1 30.7 ## p_loo 204.2 7.8 ## looic 4736.3 61.4 ## ------ ## Monte Carlo SE of elpd_loo is 0.2. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 415 99.3% 524 ## (0.5, 0.7] (ok) 3 0.7% 965 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Lets inspect the summary. All posteriors include 0, only satisfaction (within) comes close. summary(model6, priors = TRUE) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: error ~ 1 + autonomy_state_between + competence_state_between + relatedness_state_between + satisfied_between + boring_between + stressful_between + enjoyable_between + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within + (1 + autonomy_state_within + competence_state_within + relatedness_state_within + satisfied_within + boring_within + stressful_within + enjoyable_within | id) + (1 | day) ## Data: study1 (Number of observations: 418) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b ~ normal(0, 25) ## Intercept ~ student_t(10, 20, 100) ## L ~ lkj_corr_cholesky(1) ## nu ~ gamma(2, 0.1) ## sd ~ student_t(3, 0, 56.4) ## sigma ~ student_t(3, 0, 56.4) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 3.42 3.53 0.12 12.17 1.00 7456 7118 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 61.71 6.13 50.50 74.40 1.00 2533 5226 ## sd(autonomy_state_within) 6.08 3.99 0.35 14.85 1.00 3444 5438 ## sd(competence_state_within) 3.93 2.98 0.16 11.07 1.00 5720 6710 ## sd(relatedness_state_within) 9.65 6.50 0.45 23.82 1.00 1877 3608 ## sd(satisfied_within) 7.20 4.24 0.43 16.23 1.00 2343 4739 ## sd(boring_within) 2.93 2.28 0.10 8.42 1.00 4427 5807 ## sd(stressful_within) 4.68 3.13 0.21 11.56 1.00 1907 4368 ## sd(enjoyable_within) 3.54 2.65 0.15 9.98 1.00 4206 6088 ## cor(Intercept,autonomy_state_within) 0.03 0.31 -0.57 0.61 1.00 11743 8439 ## cor(Intercept,competence_state_within) 0.00 0.33 -0.63 0.64 1.00 15909 8803 ## cor(autonomy_state_within,competence_state_within) -0.05 0.34 -0.68 0.60 1.00 12817 9811 ## cor(Intercept,relatedness_state_within) -0.14 0.31 -0.68 0.51 1.00 7624 8454 ## cor(autonomy_state_within,relatedness_state_within) -0.05 0.33 -0.66 0.60 1.00 7309 8183 ## cor(competence_state_within,relatedness_state_within) 0.00 0.33 -0.62 0.63 1.00 7938 9181 ## cor(Intercept,satisfied_within) -0.20 0.30 -0.71 0.44 1.00 9855 8185 ## cor(autonomy_state_within,satisfied_within) -0.05 0.33 -0.67 0.61 1.00 6581 8107 ## cor(competence_state_within,satisfied_within) -0.05 0.33 -0.66 0.61 1.00 6539 8703 ## cor(relatedness_state_within,satisfied_within) -0.03 0.33 -0.64 0.61 1.00 7108 8023 ## cor(Intercept,boring_within) -0.01 0.33 -0.62 0.62 1.00 15554 9548 ## cor(autonomy_state_within,boring_within) -0.01 0.33 -0.64 0.62 1.00 12145 9704 ## cor(competence_state_within,boring_within) 0.02 0.33 -0.61 0.64 1.00 10078 9187 ## cor(relatedness_state_within,boring_within) -0.00 0.33 -0.63 0.63 1.00 11403 10241 ## cor(satisfied_within,boring_within) 0.01 0.33 -0.63 0.63 1.00 11135 10441 ## cor(Intercept,stressful_within) 0.03 0.30 -0.56 0.60 1.00 12347 8296 ## cor(autonomy_state_within,stressful_within) 0.08 0.34 -0.58 0.69 1.00 6490 7890 ## cor(competence_state_within,stressful_within) 0.01 0.33 -0.63 0.64 1.00 6964 8863 ## cor(relatedness_state_within,stressful_within) 0.00 0.33 -0.62 0.62 1.00 7347 9079 ## cor(satisfied_within,stressful_within) -0.02 0.33 -0.63 0.60 1.00 6808 9360 ## cor(boring_within,stressful_within) -0.01 0.33 -0.62 0.62 1.00 7663 9685 ## cor(Intercept,enjoyable_within) -0.04 0.32 -0.63 0.59 1.00 16828 8556 ## cor(autonomy_state_within,enjoyable_within) -0.04 0.33 -0.66 0.61 1.00 12316 9356 ## cor(competence_state_within,enjoyable_within) -0.05 0.34 -0.69 0.61 1.00 10687 9355 ## cor(relatedness_state_within,enjoyable_within) -0.04 0.33 -0.66 0.60 1.00 9867 9262 ## cor(satisfied_within,enjoyable_within) -0.06 0.33 -0.67 0.60 1.00 10953 10778 ## cor(boring_within,enjoyable_within) 0.05 0.34 -0.60 0.68 1.00 9391 10550 ## cor(stressful_within,enjoyable_within) 0.05 0.33 -0.60 0.66 1.00 8960 9976 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 86.88 77.56 -65.70 238.81 1.00 2752 4530 ## autonomy_state_between -3.04 11.00 -24.81 18.66 1.00 3174 5822 ## competence_state_between -9.97 9.92 -29.55 9.69 1.00 3299 5130 ## relatedness_state_between 5.55 9.55 -13.26 24.35 1.00 3147 5114 ## satisfied_between -13.00 11.32 -35.48 9.26 1.00 2544 4848 ## boring_between 4.97 6.81 -8.76 18.30 1.00 2619 4431 ## stressful_between -7.55 5.83 -18.90 3.92 1.00 3106 5140 ## enjoyable_between 6.88 11.12 -14.52 28.89 1.00 2708 5134 ## autonomy_state_within -0.25 3.34 -6.83 6.22 1.00 11063 9397 ## competence_state_within -0.35 3.15 -6.57 5.92 1.00 10688 8919 ## relatedness_state_within 0.48 4.28 -8.21 8.64 1.00 7366 8089 ## satisfied_within -6.06 3.09 -12.11 0.06 1.00 9937 9359 ## boring_within 2.70 2.01 -1.17 6.75 1.00 9306 9585 ## stressful_within -0.49 1.95 -4.34 3.40 1.00 10024 9298 ## enjoyable_within 2.63 2.57 -2.37 7.69 1.00 9981 9549 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 20.25 2.41 15.96 25.44 1.00 3054 5285 ## nu 1.16 0.11 1.01 1.42 1.00 6060 6060 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.58) visually confirms what I wrote above. Note the difference in scale: everything except the correlations are in percent error, whereas the correlations are on their -1 to 1 scale. Figure 4.58: Effects plot for Model 6 4.5 Does social media use predict well-being? For this RQ, we have three questions blocks. We want to know how day-level social media use (both types) and accuracy are related to well-being. The three questions are: 1. Does objective-only engagement predict day-level well-being? 2. Does subjective-only engagement predict day-level well-being? 3. Does accuracy predict day-level well-being? 4.5.1 Model 7: Objective use predicting well-being For this model, well predict well-being on that day with objective social media use on that day. Priors here can be controversial, depending on what literature we look to. Well-being is often left-skewed, so we could go for a skewed normal distribution for the model. However, that might be too strong an assumption, which is why Ill use a model that assumes a normal Gaussian distribution. As for the specific priors: When social media use is at zero between people and at zero within, Ill simply assume a normal distribution centered on the midpoint of the scale (i.e., 3). As for the SD of that distribution, we know the bounds of the scale, so an SD of 1 will have 95% of cases within 1 and 5, which is exactly what we want. As for the slopes: There are two papers I know of that found a negative, small effect, but another paper that found a negligent one. The larger literature on self-reported media use and well-being also finds very small negative effects, in the range of \\(\\beta\\) = .05. Were on the unstandardized scale here. So if we assume that well-being is at the midpoint of the scale (i.e., 3), the maximum effect an increase in social media time could have is to bring well-being to its ceiling or floor. That would imply a standard deviation of 1 again - that depends on the scale of the predictor, though. Right now, its in minutes and wed need to scale the prior accordingly. This paper found that with each increase of one standard deviation in social media time, well-being on a 7-point Likert-scale went down by -.06 units. The standard deviation was 190. Therefore (with some crude math), one hour of social media use was associated with 190/60 * -.06 = -0.19 raw units on the outcome scale. From the literature we also know that massive (unstandardized) effects are rare if impossible. Therefore, Ill center the slope distribution on a small negative value. To be conservative, Ill take about 75% of the -.19 we found above (say -0.15 Likert-scales) with a somewhat tight standard deviation (say 0.3). This way, 95% of effects will be within -.75 (-.15 + (2 times 0.2)) and 0.45 Likert-points on the outcome scale. Note that were on the between-level here: a user with one more hour of social media use will report, on average, a .15 lower score on well-being than someone else with an hour less. As for the within-effect, most research points toward negligible within-effects, and all of them are on self-reported media use. So here Ill assume a slopes that varies around zero, but Ill allow a wider standard deviation than for the between-effect, 0.4. That way, the average within-person effect will be zero and it assumes that 95% of effects are within -0.8 and + 0.8 Likert-points. For everything else, Ill once more go with the default brms priors. Lets set those priors. Note that were still on the minute scale for the predictor, but I specified priors above for hours. Thats why I transform the social media variables to hours (and center) to make interpretation easier. That also helps us set the priors we specified above. # create hour variables study1 &lt;- study1 %&gt;% mutate( across( c(social_media_objective, social_media_subjective), ~ .x / 60, # divide by 60 to get hours .names = &quot;{col}_hours&quot; ) ) # center to get between and within variables study1 &lt;- study1 %&gt;% group_by(id) %&gt;% mutate( across( ends_with(&quot;hours&quot;), list( between = ~ mean(.x, na.rm = TRUE), within = ~.x - mean(.x, na.rm = TRUE) ) ) ) %&gt;% ungroup() # set priors priors_model7 &lt;- c( # intercept prior(normal(3, 1), class = Intercept), # slopes for between prior(normal(-0.15, 0.30), class = b, coef = &quot;social_media_objective_hours_between&quot;), # slopes for between prior(normal(0, 0.40), class = b, coef = &quot;social_media_objective_hours_within&quot;) ) Okay, lets run the model. model7 &lt;- brm( data = study1, family = gaussian, prior = priors_model7, well_being_state ~ 1 + social_media_objective_hours_between + social_media_objective_hours_within + ( 1 + social_media_objective_hours_within | id ) + (1 |day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list( adapt_delta = 0.99 ), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model7_study1&quot;) ) Overall, the traceplots look fine and the chains seem to have mixed well, see (Figure ??). Figure 4.59: Traceplots and posterior distributions for Model 7 Figure 4.60: Traceplots and posterior distributions for Model 7 The posterior predictive checks (Figure 4.61) look excellent (no surprise with only one predictor). Figure 4.61: Posterior predictive checks for Model 7 One case is flagged as potentially influential, but unproblematic when calculated directly. loo(model7, reloo = TRUE) ## 1 problematic observation(s) found. ## The model will be refit 1 times. ## ## Fitting model 1 out of 1 (leaving out observation 65) ## Start sampling ## ## Computed from 12000 by 425 log-likelihood matrix ## ## Estimate SE ## elpd_loo -370.4 16.1 ## p_loo 79.0 5.3 ## looic 740.7 32.2 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 410 96.5% 1038 ## (0.5, 0.7] (ok) 15 3.5% 569 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Lets inspect the summary. The relation is estimated to be extremely close to zero, so really no interpretative wiggle room there. I think this is pretty convincing evidence for the lack of an effect. summary(model7, priors = TRUE) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: well_being_state ~ 1 + social_media_objective_hours_between + social_media_objective_hours_within + (1 + social_media_objective_hours_within | id) + (1 | day) ## Data: study1 (Number of observations: 425) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b_social_media_objective_hours_between ~ normal(-0.15, 0.3) ## b_social_media_objective_hours_within ~ normal(0, 0.4) ## Intercept ~ normal(3, 1) ## L ~ lkj_corr_cholesky(1) ## sd ~ student_t(3, 0, 2.5) ## sigma ~ student_t(3, 0, 2.5) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.17 0.11 0.05 0.47 1.00 3575 5449 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.47 0.05 0.39 0.57 1.00 4536 7237 ## sd(social_media_objective_hours_within) 0.08 0.05 0.00 0.20 1.00 3112 4958 ## cor(Intercept,social_media_objective_hours_within) -0.05 0.42 -0.86 0.80 1.00 10858 6678 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.22 0.14 2.94 3.50 1.00 4255 5282 ## social_media_objective_hours_between 0.01 0.04 -0.07 0.08 1.00 5089 6626 ## social_media_objective_hours_within -0.02 0.04 -0.09 0.06 1.00 15746 8839 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.52 0.02 0.48 0.56 1.00 10146 8819 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.62) visually confirms what I wrote above. Figure 4.62: Effects plot for Model 7 4.5.2 Model 8: Subjective use predicting well-being Next, well predict well-being on that day with subjective social media use on that day. Theres conflicting information in the literature: Im aware of a paper that says subjective estimates overstate the relation, but of another that says the opposite. Therefore, The specific priors: For the intercept, Ill use the same prior as with Model 7. As for the slopes: This paper reports median effect sizes across all specifications of \\(\\beta\\) = -.08, with effect sizes larger on the between-level, but small to negligible on the within-level. Thats among adolescents and for a longer time frame. This explicitly compared subjective to objective and found that subjective is a much stronger predictor (up to four times the effect size). This says subjective estimates ever so slightly understimate relations between use and well-being, though. Overall, Id say we can expect a somewhat more negative between-person relation, which would lead us to believe a mean negative effect of -.20 Likert-points on well-being and 0.4 standard deviation to allow for more extreme slopes. That would bring 95% of effects within -1 and 0.6 Likert-points, which is rather generous for media effects. For the within-effect, Ill stick to the same prior as for the previous model. For everything else, Ill once more go with the default brms priors. Lets set those priors. # set priors priors_model8 &lt;- c( # intercept prior(normal(3, 1), class = Intercept), # slopes for between prior(normal(-0.2, 0.4), class = b, coef = &quot;social_media_subjective_hours_between&quot;), # slopes for between prior(normal(0, 0.40), class = b, coef = &quot;social_media_subjective_hours_within&quot;) ) Okay, lets run the model. model8 &lt;- brm( data = study1, family = gaussian, prior = priors_model8, well_being_state ~ 1 + social_media_subjective_hours_between + social_media_subjective_hours_within + ( 1 + social_media_subjective_hours_within | id ) + (1 |day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list( adapt_delta = 0.99 ), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model8_study1&quot;) ) Overall, the traceplots look fine and the chains seem to have mixed well, see (Figure ??). Figure 4.63: Traceplots and posterior distributions for Model 8 Figure 4.64: Traceplots and posterior distributions for Model 8 The posterior predictive checks (Figure 4.65) look excellent, just like with Model 7 (no surprise with only one predictor). Figure 4.65: Posterior predictive checks for Model 8 One case is flagged as potentially influential, but unproblematic after being calculated directly.. loo(model8, reloo = TRUE) ## 1 problematic observation(s) found. ## The model will be refit 1 times. ## ## Fitting model 1 out of 1 (leaving out observation 413) ## Start sampling ## ## Computed from 12000 by 425 log-likelihood matrix ## ## Estimate SE ## elpd_loo -371.1 16.2 ## p_loo 78.8 5.4 ## looic 742.2 32.4 ## ------ ## Monte Carlo SE of elpd_loo is 0.2. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 405 95.3% 389 ## (0.5, 0.7] (ok) 20 4.7% 554 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Lets inspect the summary. The within effect is estimated to be negative, but extremely small. Also, the 95% CI includes zero, so we cant be sure the effect is meaningful. summary(model8, priors = TRUE) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: well_being_state ~ 1 + social_media_subjective_hours_between + social_media_subjective_hours_within + (1 + social_media_subjective_hours_within | id) + (1 | day) ## Data: study1 (Number of observations: 425) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b_social_media_subjective_hours_between ~ normal(-0.2, 0.4) ## b_social_media_subjective_hours_within ~ normal(0, 0.4) ## Intercept ~ normal(3, 1) ## L ~ lkj_corr_cholesky(1) ## sd ~ student_t(3, 0, 2.5) ## sigma ~ student_t(3, 0, 2.5) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.18 0.13 0.05 0.51 1.00 2790 3833 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.47 0.05 0.39 0.57 1.00 3971 6987 ## sd(social_media_subjective_hours_within) 0.05 0.03 0.00 0.13 1.00 3070 4785 ## cor(Intercept,social_media_subjective_hours_within) 0.23 0.47 -0.83 0.95 1.00 8042 5984 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.27 0.14 2.99 3.55 1.00 3189 4864 ## social_media_subjective_hours_between -0.01 0.04 -0.08 0.06 1.00 3174 5245 ## social_media_subjective_hours_within -0.03 0.03 -0.08 0.02 1.00 12266 8541 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.52 0.02 0.48 0.56 1.00 9007 8503 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.66) shows that the effect, if even there, is small at best. Figure 4.66: Effects plot for Model 8 4.5.3 Model 9: Accuracy predicting well-being For this model, we ask whether the error people make in their estimation is related to their well-being on the state level. As for as I know, theres little prior knowledge we could build on for the relation between accuracy and well-being. As for the specific priors: Like before, at perfect accuracy (0% error) and zero deviation Ill assume a normal distribution centered on the midpoint of the scale (i.e., 3) with the intercept. Ill set the SD of that prior distribution to 1 once more. As for the slopes: Theres one paper that correlated the absolute discrepancy between subjective and objective social media use and well-being indicators aggregated over a week. They found a \\(\\beta\\) = .16 between log-transformed discrepancies and depression scores. Discrepancy is a different measurement than the error that we calculated for accuracy. Therefore, Ill use a prior thats centered on a negative relationship with a small SD. Like above, I dont expect that there will be large effects just based on the literature. So Ill assume a small effect (say -0.20 Likert-scales) with a somewhat wider standard deviation than in previous models (say 0.4). This way, 95% of effects will be within -1 (-.20 + (2 times 0.3)) and 0.60 Likert-points on the outcome scale. However, error is in percent, so we wouldnt expect -.20 Likert-points with every one-percent increase. Rather, Id say the above effect is easier to specify for a standard deviation change in error, which is why I standardize error: one standard deviation increase in accuracy will be associated with, on average, a .20 lower score on well-being on the between level. I have no information for the within-level, which is why I use a prior centered on zero with the same wide standard deviation. For everything else, Ill once more go with the default brms priors. Lets set those priors and center at the same time. # standardize error study1 &lt;- study1 %&gt;% mutate( error_s = scale(error, center = FALSE, scale = TRUE) ) # center to get between and within study1 &lt;- study1 %&gt;% group_by(id) %&gt;% mutate( across( error_s, list( between = ~ mean(.x, na.rm = TRUE), within = ~.x - mean(.x, na.rm = TRUE) ) ) ) %&gt;% ungroup() # set priors priors_model9 &lt;- c( # intercept prior(normal(3, 1), class = Intercept), # slopes for between prior(normal(-0.2, 0.4), class = b, coef = &quot;error_s_between&quot;), # slopes for between prior(normal(0, 0.40), class = b, coef = &quot;error_s_within&quot;) ) And run the model. model9 &lt;- brm( data = study1, family = gaussian, prior = priors_model9, well_being_state ~ 1 + error_s_between + error_s_within + ( 1 + error_s_within | id ) + (1 |day), iter = 5000, warmup = 2000, chains = 4, cores = 4, seed = 42, control = list( adapt_delta = 0.999 ), file = here(&quot;models&quot;, &quot;study1&quot;, &quot;model9_study1&quot;) ) Overall, the traceplots look fine and the chains seem to have mixed well, see (Figure ??). Figure 4.67: Traceplots and posterior distributions for Model 9 Figure 4.68: Traceplots and posterior distributions for Model 9 The posterior predictive checks (Figure 4.69) look excellent (no surprise with only one predictor). Figure 4.69: Posterior predictive checks for Model 9 Six cases are flagged as potentially influential, which is why we calculate ELPD directly. loo(model9, reloo = TRUE) ## 6 problematic observation(s) found. ## The model will be refit 6 times. ## ## Fitting model 1 out of 6 (leaving out observation 23) ## ## Fitting model 2 out of 6 (leaving out observation 24) ## ## Fitting model 3 out of 6 (leaving out observation 255) ## ## Fitting model 4 out of 6 (leaving out observation 289) ## ## Fitting model 5 out of 6 (leaving out observation 397) ## ## Fitting model 6 out of 6 (leaving out observation 417) ## Start sampling ## Start sampling ## Start sampling ## Start sampling ## Start sampling ## Start sampling ## ## Computed from 12000 by 423 log-likelihood matrix ## ## Estimate SE ## elpd_loo -367.7 16.4 ## p_loo 81.3 5.9 ## looic 735.4 32.8 ## ------ ## Monte Carlo SE of elpd_loo is 0.2. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 407 96.2% 47 ## (0.5, 0.7] (ok) 16 3.8% 492 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Lets inspect the summary. The within effect is estimated to be negative, but really small. Also, the 95% CI includes zero and quite wide, so we cant be sure the effect is meaningful. summary(model9, priors = TRUE) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: well_being_state ~ 1 + error_s_between + error_s_within + (1 + error_s_within | id) + (1 | day) ## Data: study1 (Number of observations: 423) ## Samples: 4 chains, each with iter = 5000; warmup = 2000; thin = 1; ## total post-warmup samples = 12000 ## ## Priors: ## b_error_s_between ~ normal(-0.2, 0.4) ## b_error_s_within ~ normal(0, 0.4) ## Intercept ~ normal(3, 1) ## L ~ lkj_corr_cholesky(1) ## sd ~ student_t(3, 0, 2.5) ## sigma ~ student_t(3, 0, 2.5) ## ## Group-Level Effects: ## ~day (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.19 0.15 0.06 0.53 1.00 3196 3812 ## ## ~id (Number of levels: 94) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.47 0.04 0.39 0.56 1.00 4652 8200 ## sd(error_s_within) 0.14 0.09 0.01 0.34 1.00 2730 5295 ## cor(Intercept,error_s_within) 0.25 0.42 -0.72 0.93 1.00 9691 7015 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.25 0.13 3.01 3.48 1.00 4066 3419 ## error_s_between -0.02 0.07 -0.16 0.12 1.00 6313 7487 ## error_s_within -0.03 0.06 -0.15 0.08 1.00 13197 8720 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.51 0.02 0.48 0.56 1.00 9411 9387 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The effects plot (Figure 4.70) shows that the effect, if even there, is small at best. Figure 4.70: Effects plot for Model 9 "],["synthesis-study-1.html", "5 Synthesis Study 1 5.1 Personality on smartphone use (trait) 5.2 Experiences on smartphone use (state) 5.3 Smartphone use on well-being (state)", " 5 Synthesis Study 1 Alright, here I visualize the findings from the analysis section. Ill create a summary figure for each of the three blocks. For that, Ill extract the model estimates of the effects from each model first. model1 &lt;- read_rds(here(&quot;models&quot;, &quot;study1&quot;, &quot;model1_study1.rds&quot;)) model2 &lt;- read_rds(here(&quot;models&quot;, &quot;study1&quot;, &quot;model2_study1.rds&quot;)) model3 &lt;- readRDS(here(&quot;models&quot;, &quot;study1&quot;, &quot;model3_study1.rds&quot;)) model4 &lt;- read_rds(here(&quot;models&quot;, &quot;study1&quot;, &quot;model4_study1.rds&quot;)) model5 &lt;- read_rds(here(&quot;models&quot;, &quot;study1&quot;, &quot;model5_study1.rds&quot;)) model6 &lt;- readRDS(here(&quot;models&quot;, &quot;study1&quot;, &quot;model6_study1.rds&quot;)) model7 &lt;- readRDS(here(&quot;models&quot;, &quot;study1&quot;, &quot;model7_study1.rds&quot;)) model8 &lt;- read_rds(here(&quot;models&quot;, &quot;study1&quot;, &quot;model8_study1.rds&quot;)) model9 &lt;- read_rds(here(&quot;models&quot;, &quot;study1&quot;, &quot;model9_study1.rds&quot;)) First we get all estimates. estimates &lt;- tibble( block = as.character(), model = as.character(), predictor = as.character(), estimate = as.double(), ci_low = as.double(), ci_high = as.double(), outcome = as.character() ) models &lt;- paste0( &quot;model&quot;, 1:9 ) for (model in models) { m &lt;- get(model) dat &lt;- fixef(m) %&gt;% # get fixed effects as_tibble(., rownames = &quot;predictor&quot;) %&gt;% # turn to tibble and save predictor variable filter(predictor != &quot;Intercept&quot;) %&gt;% # remove intercept select(-Est.Error) %&gt;% # drop variable mutate( # add the outcome by turning formula into character and selecting the first section outcome = as.character(m$formula)[1] %&gt;% str_replace(., &quot;\\n&quot;, &quot;&quot;) %&gt;% # remove line breaks str_remove(., &quot;\\\\ (.*)&quot;) # select the outcome name ) %&gt;% rename( estimate = Estimate, ci_low = `Q2.5`, ci_high = `Q97.5` ) %&gt;% mutate( model = model, # original name # block of the RQ block = case_when( parse_number(model) %in% 1:3 ~ &quot;Personality on smartphone use&quot;, parse_number(model) %in% 4:6 ~ &quot;Experiences on smartphone use&quot;, parse_number(model)%in% 7:9 ~ &quot;Smartphone use on well-being&quot; ) ) # bind them all into a tibble estimates &lt;- bind_rows( estimates, dat ) } Next, lets make some cosmetic changes. estimates &lt;- estimates %&gt;% # get odds for estimates of the gamma models mutate( across( estimate:ci_high, ~ case_when( str_detect(outcome, &quot;social_media&quot;) ~ exp(.x), TRUE ~ .x ) ) ) %&gt;% # create variable that indicates within or between for models on the state level and whether predictors are state or trait mutate( between_within = case_when( str_detect(predictor, &quot;between&quot;) ~ &quot;between&quot;, str_detect(predictor, &quot;within&quot;) ~ &quot;within&quot;, TRUE ~ NA_character_ ), trait_state = case_when( block != &quot;Personality on smartphone use&quot; ~ &quot;state&quot;, TRUE ~ &quot;trait&quot; ) ) %&gt;% # clean up level names mutate( across( c(predictor, outcome), ~ case_when( str_detect(.x, &quot;objective&quot;) ~ &quot;Objective&quot;, str_detect(.x, &quot;subjective&quot;) ~ &quot;Subjective&quot;, str_detect(.x, &quot;_c$|between|within&quot;) ~ str_to_title(str_remove(.x, &quot;_.*&quot;)), str_detect(.x, &quot;well&quot;) ~ &quot;Well-being&quot;, TRUE ~ str_to_title(.x) ) ) ) %&gt;% # give a value for the line that signifies no effect mutate( line = if_else(outcome %in% c(&quot;Objective&quot;, &quot;Subjective&quot;), 1, 0) ) 5.1 Personality on smartphone use (trait) Okay, next I create the first summary figure. We see that none of the trait underlying true trait relations (conditional on our model) exclude zero. Neuroticism and openness might be related to less error in estimating smartphone use, but even their credible interval includes zero. # function for breaks my_breaks &lt;- function(x) { if (max(x) &gt; 5){ c(-50, 0, 50) } else { c(0.8, 1, 1.2) } } # function for limits my_limits &lt;- function(x) { if (max(x) &gt; 5){ c(-50, 50) } else { c(0.65, 1.35) } } # plot ggplot( estimates %&gt;% filter(block == &quot;Personality on smartphone use&quot;) %&gt;% mutate( outcome = as.factor(outcome), outcome = fct_recode( outcome, &quot;Objective (min)&quot; = &quot;Objective&quot;, &quot;Subjective (min)&quot; = &quot;Subjective&quot;, &quot;Accuracy (%)&quot; = &quot;Error&quot; ) ), aes( x = estimate, y = predictor ) ) + geom_vline( aes( xintercept = line ), color = &quot;#999999&quot;, linetype = &quot;dashed&quot; ) + geom_pointrange( aes( xmin = ci_low, xmax = ci_high )#, # position = position_dodge2(.6, reverse = TRUE) ) + facet_wrap( ~ outcome, scales = &quot;free_x&quot; ) + scale_x_continuous(breaks = my_breaks, limits = my_limits) + theme( axis.title.y = element_blank(), axis.title.x = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank(), strip.background.x = element_blank(), strip.background.y = element_blank(), text = element_text(family = &quot;Corbel&quot;) ) -&gt; figure4 figure4 ggsave( here(&quot;figures&quot;, &quot;figure4.tiff&quot;), plot = figure4, width = 21 * 0.8, height = 29.7 * 0.4, units = &quot;cm&quot;, dpi = 300 ) 5.2 Experiences on smartphone use (state) Ill move on to the next block investigating state relations between experiences and smartphone use. ggplot( estimates %&gt;% filter(block == &quot;Experiences on smartphone use&quot;) %&gt;% mutate( outcome = as.factor(outcome), outcome = fct_recode( outcome, &quot;Objective (min)&quot; = &quot;Objective&quot;, &quot;Subjective (min)&quot; = &quot;Subjective&quot;, &quot;Accuracy (%)&quot; = &quot;Error&quot; ) ), aes( x = estimate, y = predictor, color = between_within ) ) + geom_vline( aes( xintercept = line ), color = &quot;#999999&quot;, linetype = &quot;dashed&quot; ) + geom_pointrange( aes( xmin = ci_low, xmax = ci_high ), position = position_dodge2(.6, reverse = TRUE) ) + facet_wrap( ~ outcome, scales = &quot;free_x&quot; ) + scale_x_continuous(breaks = my_breaks, limits = my_limits) + scale_color_manual(values = c(&quot;#009E73&quot;, &quot;#D55E00&quot;)) + scale_fill_manual(values = c(&quot;#009E73&quot;, &quot;#D55E00&quot;)) + theme( axis.title.y = element_blank(), axis.title.x = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank(), strip.background.x = element_blank(), strip.background.y = element_blank(), legend.title = element_blank(), legend.position = &quot;bottom&quot;, text = element_text(family = &quot;Corbel&quot;) ) -&gt; figure5 figure5 ggsave( here(&quot;figures&quot;, &quot;figure5.tiff&quot;), plot = figure5, width = 21 * 0.8, height = 29.7 * 0.4, units = &quot;cm&quot;, dpi = 300 ) 5.3 Smartphone use on well-being (state) Ill move on to the next block investigating state relations between smartphone use and well-being. ggplot( estimates %&gt;% filter(block == &quot;Smartphone use on well-being&quot;) %&gt;% mutate( predictor = as.factor(predictor), predictor = fct_recode( predictor, &quot;Objective (h)&quot; = &quot;Objective&quot;, &quot;Subjective (h)&quot; = &quot;Subjective&quot;, &quot;Accuracy (%, standardized)&quot; = &quot;Error&quot; ) ), aes( x = estimate, y = predictor, color = between_within ) ) + geom_vline( aes( xintercept = line ), color = &quot;#999999&quot;, linetype = &quot;dashed&quot; ) + geom_pointrange( aes( xmin = ci_low, xmax = ci_high ), position = position_dodge2(.6, reverse = TRUE) ) + scale_x_continuous(breaks = c(-0.2, 0, 0.2), limits = c(-0.21, 0.21)) + scale_color_manual(values = c(&quot;#009E73&quot;, &quot;#D55E00&quot;)) + scale_fill_manual(values = c(&quot;#009E73&quot;, &quot;#D55E00&quot;)) + theme( axis.title.y = element_blank(), axis.title.x = element_blank(), axis.ticks.y = element_blank(), axis.line.y = element_blank(), strip.background.x = element_blank(), strip.background.y = element_blank(), legend.title = element_blank(), legend.position = &quot;bottom&quot;, text = element_text(family = &quot;Corbel&quot;) ) -&gt; figure6 figure6 ggsave( here(&quot;figures&quot;, &quot;figure6.tiff&quot;), plot = figure6, width = 21 * 0.8, height = 29.7 * 0.4, units = &quot;cm&quot;, dpi = 300 ) "]]
