[
["index.html", "Analysis Report Preface", " Analysis Report Niklas Johannes 2020-06-24 Preface On this site, I document all steps, from data processing to final analyses, for the project [project title.] "],
["setting-up-study1.html", "1 Setting up", " 1 Setting up In this section, I process and analyze the data for Study 1. First, I load all libraries that we need for the analysis. The pacman package just makes it easier to load packages. Note that the final version of this page doesn’t evaluate the code chunk below to avoid installing packages on your machine. If you’re fine with them being installed, set eval = TRUE. if (!requireNamespace(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) library(pacman) # load packages p_load( tidyverse, lubridate, here, MBESS, ggridges, GGally, ggalt, cowplot ) # set seed set.seed(42) # set theme theme_set(theme_cowplot()) "],
["data-processing-study1.html", "2 Data processing Study 1 2.1 Personality data 2.2 Diary data 2.3 Screen time data 2.4 Merge data sets", " 2 Data processing Study 1 Let’s process the raw data and get them to a format that we can work with for the analysis. Study 1 has three raw data files: personality_raw.csv is the survey participants filled out at the beginning of the study, reporting personality traits diary_raw.csv is the end of day survey participants filled out during the diary part of the study screen_time_raw.csv is the objective screen time data participants reported 2.1 Personality data Let’s load the personality data. personality_raw &lt;- read_csv(here(&quot;data&quot;, &quot;study1&quot;, &quot;personality_raw.csv&quot;)) Qualtrics names aren’t very informative, so I give some sensible variable names (although this data set has mostly meaningful variable names). Also, there are variables (i.e., anything starting with afi to selfreflect) that were part of a different study, which is why I don’t include them here. Note that I don’t want to change the raw data object, so I’ll keep using personality as a new object from here on. personality &lt;- personality_raw %&gt;% select( # meta data start_date = StartDate, end_date = EndDate, progress = Progress, duration = Duration__in_seconds_, finished = Finished, recorded_date = RecordedDate, id = study_id, date, # basic psychological need satisfaction trait bpns_ = starts_with(&quot;bpns&quot;), # big five big_five_ = starts_with(&quot;bfi&quot;), # demographics gender, gender_entry = gender_3_TEXT, age = Q42, ethnicity_american_indian = ethnicity_1, ethnicity_asian = ethnicity_2, ethnicity_island = ethnicity_3, ethnicity_black = ethnicity_4, ethnicity_white = ethnicity_5, ethnicity_hispanic = ethnicity_6, ethnicity_multi = ethnicity_7 ) Next, I make sure those variables have the correct variable type. I’ll also recode factor levels so that labels are readable. Note that participants filled in their age from a dropdown menu to prevent typos. The dropdown went from age 18 (1) to 30+ (13). Nobody was older than thirty, so I’ll recode age as well. personality &lt;- personality %&gt;% mutate( id = paste0(&quot;pp_&quot;, id), # add pp for participant just so later I don&#39;t mistake the id variable for a count across( # turn those two into factors c(finished, id, gender), as.factor ), finished = fct_recode( finished, &quot;yes&quot; = &quot;1&quot;, &quot;no&quot; = &quot;0&quot; ), gender = fct_recode( gender, &quot;male&quot; = &quot;1&quot;, &quot;female&quot; = &quot;2&quot;, &quot;other&quot; = &quot;3&quot; ), age = age + 17 ) I see that ethnicity was multiple choice although “multiracial” is an answer option. So I think it makes more sense to combine this all into one variable. That required quite a lot of code, but was still less problematic than transforming the ethnicity variables to long format. I took the following steps: check who reported multiple ethnicities those who did get an NA on all other ethnicity variables recoded the variables from numeric to containing the ethnicity factor level and coalesce personality &lt;- personality %&gt;% mutate( multiple = rowSums(select(., starts_with(&quot;ethnicity&quot;)), na.rm = TRUE), # check who reported multiple ethnicities # those with multiple get NA for all other ethnicity variables across( c(ethnicity_american_indian:ethnicity_hispanic), ~ case_when( multiple &gt; 1 ~ NA_real_, TRUE ~ . ) ), # turn into character which makes recoding below easier across( c(ethnicity_american_indian:ethnicity_hispanic), as.character ), # recode the individual variables ethnicity_american_indian = if_else(ethnicity_american_indian == 1, &quot;American Indian/Alaska Native&quot;, NA_character_), ethnicity_asian = if_else(ethnicity_asian == 1, &quot;Asian&quot;, NA_character_), ethnicity_island = if_else(ethnicity_island == 1, &quot;Native Hawaiian or Other Pacific Islander&quot;, NA_character_), ethnicity_black = if_else(ethnicity_black == 1, &quot;Black or African American&quot;, NA_character_), ethnicity_white = if_else(ethnicity_white == 1, &quot;White&quot;, NA_character_), ethnicity_hispanic = if_else(ethnicity_hispanic == 1, &quot;Hispanic or Latino&quot;, NA_character_), ethnicity_multi = if_else(multiple &gt; 1, &quot;Multiracial&quot;, NA_character_), # coalesce them (coalesce doesn&#39;t work with .select helpers, so needed to type the variables out) ethnicity = as.factor( coalesce( ethnicity_american_indian, ethnicity_asian, ethnicity_island, ethnicity_black, ethnicity_white, ethnicity_hispanic, ethnicity_multi ) ) ) %&gt;% select(start_date:age, ethnicity) Then I remove those cases without an id (aka test runs) and those with an id but not other entries. personality &lt;- personality %&gt;% filter(id != &quot;pp_NA&quot; &amp; !is.na(start_date)) Let’s see how many missing values there are left per participant (aka row) in 2.1. Most participants filled out everything, but a couple of them have basically empty rows. A couple of participants have quite a handful of empty cells. I inspected their data and most of them just didn’t fill out the Big Five, but one participant didn’t fill out anything. Table 2.1: Count of number of missing values per row na_count_row n 0 1 1 246 2 29 3 5 4 2 7 1 9 2 25 1 49 4 74 1 I’ll exclude the latter. # exclude those with &gt; 15 missing values personality&lt;- personality %&gt;% mutate( na_count_row = rowSums(is.na(.)) ) %&gt;% filter(na_count_row &lt;=70) Alright, time to construct the means for the scales. But first, I check data quality by seeing whether there are instances of “straightlining” (i.e., choosing the same response option for all items of a scale). We also had attention checks in the survey. Let’s have a look at those attention checks. For item bpns_25 participants had to choose 2 as a response and for item big_five_45 they had to choose 5. When we check Table 2.2, we see that 5 participants didn’t pass both attention checks. Table 2.2: Failed attention checks bpns_25 != 2 n FALSE 277 TRUE 13 NA 1 big_five_45 != 5 n FALSE 275 TRUE 7 NA 9 (bpns_25 != 2 &amp; big_five_45 != 5) n FALSE 286 TRUE 5 I’ll exclude those participants because I don’t think we can trust their data. personality &lt;- personality %&gt;% filter(!(bpns_25 != 2 &amp; big_five_45 != 5) | is.na(bpns_25) | is.na(big_five_45)) # otherwise filter automatically drops rows with NA Next, I check for straightlining by inspecting the variance per daily survey on the BPNS and BF scales. Turns out that it’s not super straightforward to compute variance per row, found help here. personality &lt;- personality %&gt;% rowwise() %&gt;% mutate( bpns_sd = sd( c_across(starts_with(&quot;bpns&quot;)), na.rm = TRUE ), big_five_sd = sd( c_across(starts_with(&quot;big_five&quot;)), na.rm = TRUE ) ) %&gt;% unnest(cols = c()) If we visualize that variance in Figure 2.1, we can see that there’s little cause for concern: nobody straightlined. Figure 2.1: Distribution of variances per survey on two measures Alright, now we can aggregate the individual items to scale scores. Some of those items are reverse-coded (see the codebook for details). I’ll code all scales such that a higher scores means scoring more positively on that scale. personality &lt;- personality %&gt;% mutate( across( c( bpns_5:bpns_8, bpns_13:bpns_15, bpns_21:bpns_25, ), function(.) 8 - . ), across( c( big_five_2, big_five_6, big_five_8, big_five_9, big_five_12, big_five_18, big_five_21, big_five_23, big_five_27, big_five_31, big_five_37, big_five_43 ), function(.) 6 - . ) ) Now I create the scales by taking the mean of the respective items. Note that I remove missing values in computing row means to ignore the occasional missing value. (For those with missing values throughout, see next code chunk.) personality &lt;- personality %&gt;% mutate( # trait needs autonomy_trait = rowMeans(select(., bpns_1:bpns_8), na.rm = TRUE), competence_trait = rowMeans(select(., bpns_9:bpns_16), na.rm = TRUE), relatedness_trait = rowMeans(select(., bpns_17:bpns_24), na.rm = TRUE), # big five extraversion = rowMeans( select( ., big_five_1, big_five_6, big_five_11, big_five_16, big_five_21, big_five_26, big_five_31, big_five_36 ), na.rm = TRUE ), agreeableness = rowMeans( select( ., big_five_2, big_five_7, big_five_12, big_five_17, big_five_22, big_five_27, big_five_32, big_five_37, big_five_42 ), na.rm = TRUE ), conscientiousness = rowMeans( select( ., big_five_3, big_five_8, big_five_13, big_five_18, big_five_23, big_five_28, big_five_33, big_five_38, big_five_43 ), na.rm = TRUE ), neuroticism = rowMeans( select( ., big_five_4, big_five_9, big_five_14, big_five_19, big_five_24, big_five_29, big_five_34, big_five_39 ), na.rm = TRUE ), openness = rowMeans( select( ., big_five_5, big_five_10, big_five_15, big_five_20, big_five_25, big_five_30, big_five_35, big_five_40, big_five_41, big_five_44 ), na.rm = TRUE ) ) Last, some participants didn’t fill out the Big Five items, so they haven an NaN entry for those scales now. I’ll turn that into an NA value personality &lt;- personality %&gt;% mutate( across( c(autonomy_trait:openness), ~ na_if(.x, &quot;NaN&quot;) ) ) 2.2 Diary data Let’s load and inspect the diary data. # load data diary_raw &lt;- read_csv(here(&quot;data&quot;, &quot;study1&quot;, &quot;diary_raw.csv&quot;)) Qualtrics names aren’t very informative, so I give some sensible variable names. Again, there are many variables (i.e., anything before Q21-1) that were part of a different study, which is why I don’t include them here. Like before, I don’t want to change the raw data object, so I’ll keep using diary as a new object from here on. diary &lt;- diary_raw %&gt;% select( # meta data start_date = StartDate, end_date = EndDate, progress = Progress, duration = Duration__in_seconds_, finished = Finished, recorded_date = RecordedDate, id = study_id, day = Day, date = Q27, # social media subjective estimates hours_subjective = Q21_1, minutes_subjective = Q21_2, pickups_subjective = Q22_1, notifications_subjective = Q23_1, # arousal low_positive_peaceful = Q24_1, low_positive_calm = Q24_2, low_positive_related = Q24_3, high_negative_anxious = Q24_4, high_negative_jittery = Q24_5, high_negative_tense = Q24_6, high_positive_happy = Q24_7, high_positive_energized = Q24_8, high_positive_excited = Q24_9, low_negative_sluggish = Q24_10, low_negative_sad = Q24_11, low_negative_gloomy = Q24_12, # autonomy need satisfaction autonomy_ = Q25_1:Q25_4, # competence need satisfaction competence_ = Q25_5:Q25_8, # relatedness need satisfaction relatedness_ = Q25_9:Q25_12, # experiences satisfied = Q30_1, boring = Q30_2, stressful = Q30_3, enjoyable = Q30_4 ) Next, I make sure those variables have the correct variable type. I’ll also recode factor levels so that labels are readable. Note that day goes from 1 to 5. All participants started on a Monday, which is why I turn day into a factor and reorder the levels. diary &lt;- diary %&gt;% mutate( id = paste0(&quot;pp_&quot;, id), # add pp for participant just so later I don&#39;t mistake the id variable for a count across( # turn those two into factors c(finished, id, day), as.factor ), finished = fct_recode( finished, &quot;yes&quot; = &quot;1&quot;, &quot;no&quot; = &quot;0&quot; ), day = fct_recode( day, &quot;monday&quot; = &quot;1&quot;, &quot;tuesday&quot; = &quot;2&quot;, &quot;wednesday&quot; = &quot;3&quot;, &quot;thursday&quot; = &quot;4&quot;, &quot;friday&quot; = &quot;5&quot; ) ) Then I remove all empty rows (i.e., rows without a start date). diary &lt;- diary %&gt;% filter(!is.na(start_date)) Afterwards, I inspect how many NAs there are per row in Table 2.3. Most participants filled out everything, but a couple of them have basically empty rows (e.g., those with 30 or 40 missing values per row). Table 2.3: Count of number of missing values per row na_count_row n 0 1072 1 49 2 16 3 3 4 1 6 1 8 1 9 2 16 1 20 1 23 1 28 2 32 24 I manually inspected those with more than 15 missing values and neither of them even finished any of the scales. Therefore, I’ll exclude them here. # exclude those with &gt; 15 missing values diary &lt;- diary %&gt;% mutate( na_count_row = rowSums(is.na(.)) ) %&gt;% filter(na_count_row &lt;= 15) %&gt;% mutate(id = droplevels(id)) Alright, time to construct the means for the scales. But first, as always with numbers that participants have to fill in themselves, I’ll check for unrealistic entries and general data quality. I’ll visualize the text entries for hours and minutes on social media and notifications in Figure 2.2. Figure 2.2: Distribution of self-reported social media indicators For notifications, it’s hard to argue that it’s unrealistic that participants thought they got several hundred of them. So I will leave those unchanged. However, there are several outliers in hours_subjective and minutes_subjective. I manually inspected those cases and it’s hard to tell whether participants just mixed up hours and minutes or whether their entry is indeed not meaningful. One participant filled in 50h and 500min. Another 40h. For those two, I’ll set their subjective values to NA. One participant filled in 5h and 86min. The minutes entry might have been a typo, but it’s hard to tell what the participant meant, so I’ll set that to NA as well. diary &lt;- diary %&gt;% mutate( hours_subjective = case_when( hours_subjective &gt; 24 | minutes_subjective &gt; 60 ~ NA_real_, TRUE ~ hours_subjective ), minutes_subjective = if_else( minutes_subjective &gt; 60, NA_real_, minutes_subjective ) ) Next, I check data quality further by seeing whether there are instances of “straightlining”. For that, I check the variance per daily survey on the well-being scale, need satisfaction, and experiences. diary &lt;- diary %&gt;% rowwise() %&gt;% mutate( well_being_sd = sd( c_across(low_positive_peaceful:low_negative_gloomy), na.rm = TRUE ), needs_sd = sd( c_across(autonomy_1:relatedness_4), na.rm = TRUE ), experiences_sd = sd( c_across(satisfied:enjoyable), na.rm = TRUE ) ) %&gt;% unnest(cols = c()) If we visualize the instances where participants didn’t have any variation in their answers (aka straightlining) in Figure 2.3, we see that it’s not uncommon. There are several daily surveys where participants straightlined. Figure 2.3: Distribution of variances per survey on three scales In general, it’s not uncommon to select the same answer, especially for well-being measures. However, I’d be suspicious of those participants who straightlined for more than one scale (particularly the need satisfaction scale because it has quite a lot of items.) I’ll check how many surveys there are that have zero variance across those scales and how many participants there are who have at least one straightlining instance in more than 50% of their surveys. diary &lt;- diary %&gt;% mutate( zero_variances = rowSums( select(., ends_with(&quot;_sd&quot;)) == 0, # count instances of zero variance across the three scales na.rm = TRUE ) ) Overall, Table 2.4 shows that 94% of the sample didn’t staightline. Table 2.4: Percentage of the number of zero variance per row zero_variances n percent 0 1077 0.9406114 1 27 0.0235808 2 21 0.0183406 3 20 0.0174672 But there are a couple of participants who seemed to rush through the survey, see Table 2.5. Manually inspecting them indeed shows that it’s hard to trust their data. Table 2.5: Percentage of participants with a number of zero-variance scales id total_surveys total_straightline_surveys percentage pp_172 4 3 0.75 pp_121 5 3 0.60 pp_164 5 3 0.60 pp_169 5 3 0.60 pp_103 2 1 0.50 pp_168 2 1 0.50 pp_290 2 1 0.50 pp_154 5 2 0.40 pp_198 5 2 0.40 pp_35 5 2 0.40 I will exclude those participants and then surveys with more than one instance of straightlining. diary &lt;- diary %&gt;% filter(!id %in% c(&quot;pp_172&quot;, &quot;pp_121&quot;, &quot;pp_164&quot;, &quot;pp_169&quot;, &quot;pp_103&quot;, &quot;pp_168&quot;, &quot;pp_290&quot;)) %&gt;% filter(zero_variances &lt; 2) %&gt;% mutate( id = droplevels(id) ) Now there’s very little straightlining left (see Table 2.6 and compare to Table 2.4). Table 2.6: Percentage of the number of zero variance per row zero_variances n percent 0 1072 0.981685 1 20 0.018315 Alright, now we can aggregate the individual items to scale scores. Some of those items are reverse-coded. Namely, the third and fourth item for the need satisfaction subscales are reverse coded, as are all negative arousal items (if we want to create a score that shows higher well-being). See codebook for details. # reverse code need items and arousal diary &lt;- diary %&gt;% mutate( across( c(ends_with(&quot;_3&quot;), ends_with(&quot;_4&quot;)), function(.) 8 - . ), across( contains(&quot;negative&quot;), function(.) 6 - . ) ) I create the scales by taking the mean of the respective items. I also create a social_media_subjective measure by combining hours and minutes of estimated social media time. diary &lt;- diary %&gt;% mutate( social_media_subjective = hours_subjective * 60 + minutes_subjective, well_being_state = rowMeans(select(., low_positive_peaceful:low_negative_gloomy), na.rm = TRUE), autonomy_state = rowMeans(select(., contains(&quot;autonomy&quot;)), na.rm = TRUE), competence_state = rowMeans(select(., contains(&quot;competence&quot;)), na.rm = TRUE), relatedness_state = rowMeans(select(., contains(&quot;relatedness&quot;)), na.rm = TRUE) ) Further, participants’ first day of participation was always a Monday. Specifically, three days in April 2019: the 15th, 22nd, 29th. Let’s check whether all of them indeed have their first day on one of those days. Table 2.7 shows all day one surveys which were not recorded on one of those three dates. Indeed, everyone opened their first survey on one of the three start days. Some participants opened/recorded their answers right after midnight, which is fine. Table 2.7: Recorded dates that weren’t on a Monday id start_date recorded_date duration day pp_21 2019-04-30 00:35:48 2019-04-30 00:57:14 1284 monday pp_71 2019-04-22 16:01:29 2019-04-24 11:00:01 154711 monday pp_110 2019-04-22 20:16:35 2019-04-23 00:30:27 15231 monday pp_120 2019-04-23 00:08:15 2019-04-23 00:20:35 739 monday pp_192 2019-04-15 20:36:47 2019-04-16 00:27:47 13859 monday pp_201 2019-04-30 00:27:10 2019-04-30 00:52:59 1548 monday pp_211 2019-04-22 17:24:37 2019-04-23 12:13:59 67761 monday pp_257 2019-04-30 00:52:05 2019-04-30 01:21:55 1789 monday pp_265 2019-04-29 23:28:18 2019-04-30 01:34:43 7584 monday However, a handful of participants had their survey open for at least a day. In Table 2.8, we inspect all instances where there’s more than one day delay between opening and responding to a survey. Only pp_71 took two days to respond to a survey. Table 2.8: More than one day between begin and response to survey id start_date recorded_date duration day pp_71 2019-04-22 16:01:29 2019-04-24 11:00:01 154711 monday I guess it’s normal that participants open the survey late, forget it, and fill it out when they next day when they wake up. Because they’re students, waking up might be quite late, so I’ll check in Table 2.9 how many of them responded to a survey later than 10am the next day. Two participants were really late (i.e., responded after noon). Table 2.9: Those who filled in the survey the next day id start_date recorded_date duration day pp_53 2019-04-18 16:11:51 2019-04-19 10:08:16 64584 thursday pp_71 2019-04-22 16:01:29 2019-04-24 11:00:01 154711 monday pp_98 2019-04-30 17:21:21 2019-05-01 12:03:12 67310 tuesday pp_161 2019-05-02 18:15:49 2019-05-03 10:05:25 56975 thursday pp_162 2019-04-30 22:00:37 2019-05-01 10:52:15 46297 tuesday pp_211 2019-04-22 17:24:37 2019-04-23 12:13:59 67761 monday pp_211 2019-04-23 23:40:53 2019-04-24 10:30:27 38972 tuesday pp_244 2019-04-19 18:45:44 2019-04-20 11:52:12 61587 friday I manually checked the data from all those participants who filled out surveys so late that two surveys were recorded on the same day. pp_71 responded to the Monday survey on Wednesday, in addition to the “regular” Wednesday survey. For that reason, we can’t know whether day one refers to Monday or Tuesday. All other participants are fine: They sometimes recorded their response late, but their response pattern shows no indication that they couldn’t refer to a clear day in their response. Therefore, I remove the day one survey of pp_71 because we can’t tell to which day it refers, but leave the rest as is. diary &lt;- diary %&gt;% filter(!(id == &quot;pp_71&quot; &amp; day == &quot;monday&quot;)) 2.3 Screen time data Load the screen time data. screen_time_raw &lt;- read_csv(here(&quot;data&quot;, &quot;study1&quot;, &quot;screen_time_raw.csv&quot;)) Alright, the data are extremely wide because of the Qualtrics format. There are 220 variables in total. The data look like this: Total time over the past week of social media, plus total time per day (Q2_1 to Q10_2) Typing in the top ten social networking media (Q4_1 to Q4_10) Then participants filled in 13 items for each of those ten apps, first total time and notifications for the week for that app, then hours and minutes per day for that app (total 130 items, Q3_1 to Q76_2) Then participants reported six items per each of the ten apps, the first of which was overall pickups in the past week, followed by pickups for each day of the week (Q17_1_TEXT to Q17_10_5) Before turning the data into long format, I need to give some sensible variables names. However, manually renaming all variables is a real pain. So I’ll create the names systematically and store them in a vector, then assign them to the variables. # define days days &lt;- c( &quot;monday&quot;, &quot;tuesday&quot;, &quot;wednesday&quot;, &quot;thursday&quot;, &quot;friday&quot; ) # the list of apps apps &lt;- paste0( rep(&quot;app_&quot;, 10), 1:10 ) # add hours and minutes to apps apps_times &lt;- paste0( rep(apps, each = 6*2), &quot;_&quot;, c(&quot;hours&quot;, &quot;minutes&quot;) ) # the overall time per week and day names1 &lt;- paste0( rep(c(&quot;hours_total_&quot;, &quot;minutes_total_&quot;),6), rep(c(&quot;week&quot;, days), each = 2) ) # now app, minutes, hours, per day names2 &lt;- paste0( apps_times, &quot;_&quot;, rep(c(&quot;week&quot;, days), each = 2) ) # notifications per day names3 &lt;- paste0( apps, &quot;_&quot;, &quot;notifications&quot;, &quot;_week&quot; ) # pickups per week and day names4 &lt;- paste0( rep(apps, each = 6), &quot;_pickups_&quot;, rep(c(&quot;week&quot;, days), 6) ) Then I apply those names. Once more, I use a new object and don’t overwrite the raw data. # let&#39;s rename screen_time &lt;- screen_time_raw %&gt;% # meta-data rename( start_date = StartDate, end_date = EndDate, progress = Progress, duration = Duration__in_seconds_, finished = Finished, recorded_date = RecordedDate, id = study_id ) %&gt;% # the overall time per week and day rename_with( ~ names1, Q2_1:Q10_2 ) %&gt;% # the top ten apps rename( app_ = Q4_1:Q4_10 ) %&gt;% # now rename all app minutes and hours per day rename_with( ~ names2, c( # except the notifications because they don&#39;t fit the pattern Q3_1:Q76_2, -Q11_1, -Q22_1, -Q78_1, -Q29_1, -Q43_1, -Q36_1, -Q50_1, -Q57_1, -Q64_1, -Q71_1 ) ) %&gt;% # the past week notifications per app rename_with( ~ names3, c( Q11_1, Q22_1, Q78_1, Q29_1, Q43_1, Q36_1, Q50_1, Q57_1, Q64_1, Q71_1 ) ) %&gt;% # rename pickup variables rename_with( ~ names4, Q17_1_TEXT:Q17_10_5 ) %&gt;% # only keep variables of interest select( -Status ) # remove name objects (they were only temp files) rm(apps, apps_times, days, names1, names2, names3, names4) Before I assign variables types and factor levels, I saw that, for some reason, the past week’s number of pickups for each app have text entries, not numerical (i.e., \"twenty one\" instead of 21). I know that the english package can translate numbers to words, but not the other way around. Thank god, someone has written a function for this, see full details here. The function is in the Setting up chapter. Note that this word2num function doesn’t work well with the word “and”. For example, \"three hundred and eleven\" turns into 611, but \"three hundred eleven\" turns into the correct 311. Therefore, I first remove all “and”s from those variables, then apply the function. Similarly, I’ll replace all “a” with “one” (e.g., “a hundred” to “one hundred”). One participant also replied with a string (that they access facebook via their browser), so I’ll set that to NA. screen_time &lt;- screen_time %&gt;% mutate( across( ends_with(&quot;pickups_week&quot;), ~ case_when( .x == &quot;NA because accessed Facebook through Safari&quot; ~ NA_character_, .x == &quot;NA because accessed Twitter through Safari&quot; ~ NA_character_, TRUE ~ .x ) ), across( ends_with(&quot;pickups_week&quot;), ~ str_remove(.x, &quot;and&quot;) ), across( ends_with(&quot;pickups_week&quot;), ~ str_replace(.x, &quot;A &quot;, &quot;one &quot;) ) ) %&gt;% rowwise() %&gt;% mutate( across( ends_with(&quot;pickups_week&quot;), word2num ), across( ends_with(&quot;pickups_week&quot;), unlist ) ) %&gt;% unnest(cols = c()) Next, I assign correct variable types and factor levels. screen_time &lt;- screen_time %&gt;% mutate( id = paste0(&quot;pp_&quot;, id), across( # turn those two into factors c(finished, id), as.factor ), finished = fct_recode( finished, &quot;yes&quot; = &quot;1&quot;, &quot;no&quot; = &quot;0&quot; ) ) Then I exclude empty rows. The data set has all participant numbers even when they didn’t participate, so I’ll exclude those without a start date as well as those who didn’t fill in app_1, which means they have empty rows other than the meta-data. screen_time &lt;- screen_time %&gt;% filter(!is.na(start_date)) %&gt;% filter(!is.na(app_1)) %&gt;% mutate(id = droplevels(id)) At this point, the data set has measures on two levels: per app and time frame (day, full week), and the total times across all apps per day. I think it’s easiest to separate the data. The per app data are only informative for plotting and descriptives, but the actual data we’re interested in are the total times summed across all apps. So I’ll first get the per data app. Note that I add the names of the apps in a separate step and then calculate the objective social media time. # turn app data long apps_long &lt;- screen_time %&gt;% select( start_date:id, app_1_hours_week:app_10_pickups_friday ) %&gt;% pivot_longer( cols = c(app_1_hours_week:app_10_pickups_friday), names_to = c( &quot;app&quot;, &quot;rank&quot;, &quot;measure&quot;, &quot;time_frame&quot; ), names_sep = &quot;_&quot; ) %&gt;% pivot_wider( # then spread the measure variable which now contains hours, minutes, notifications, pickups names_from = &quot;measure&quot;, values_from = &quot;value&quot; ) %&gt;% select(-app) # redundant # get the names of the apps app_names &lt;- screen_time %&gt;% select( id, app_1:app_10 ) %&gt;% pivot_longer( -id, names_to = c(&quot;app&quot;, &quot;rank&quot;), names_sep = &quot;_&quot; ) %&gt;% select(-app) %&gt;% # redundant rename( app = value # contains name of app ) # add them to the long data set apps_long &lt;- left_join( apps_long, app_names, by = c(&quot;id&quot;, &quot;rank&quot;) ) %&gt;% relocate( app, .after = rank ) %&gt;% # if someone only filled in minutes, but not hours, turn hours to 0 mutate( hours = case_when( is.na(hours) &amp; is.na(minutes) ~ NA_real_, is.na(hours) &amp; !is.na(minutes) ~ 0, TRUE ~ hours ), # when someone only filled in hours, but not minutes, turn minutes to 0 minutes = case_when( is.na(hours) &amp; is.na(minutes) ~ NA_real_, !is.na(hours) &amp; is.na(minutes) ~ 0, TRUE ~ minutes ), # total time social_media_objective = hours * 60 + minutes ) Then I get the total times across all apps in the long format. Note that participants reported their screen time on the weekend. Thus, their reports for week will not be identical with the sum of Monday to Friday, because the screen time feature measures time on the weekend as well (aka additional screen time measured from the end of Friday until the moment they reported their weekly screen time). # get total screen time (of all social media apps in long format) totals_long &lt;- screen_time %&gt;% select( id, hours_total_week:minutes_total_friday ) %&gt;% pivot_longer( -id, names_to = c( &quot;measure&quot;, &quot;total&quot;, &quot;time_frame&quot; ), names_sep = &quot;_&quot; ) %&gt;% select(-total) %&gt;% # redundant pivot_wider( # spread hours and minutes names_from = &quot;measure&quot;, values_from = &quot;value&quot; ) %&gt;% # same as above: if one of the two measures (hours or minutes) has an entry and the other is NA, we set that NA to 0 mutate( hours = case_when( is.na(hours) &amp; is.na(minutes) ~ NA_real_, is.na(hours) &amp; !is.na(minutes) ~ 0, TRUE ~ hours ), # when someone only filled in hours, but not minutes, turn minutes to 0 minutes = case_when( is.na(hours) &amp; is.na(minutes) ~ NA_real_, !is.na(hours) &amp; is.na(minutes) ~ 0, TRUE ~ minutes ), # total time social_media_objective = hours * 60 + minutes ) The totals_long is the data set of interest because it has total screen time per day, which we can match with the diary data. Thus, to arrive at the final sample size, we’ll only keep participants who have data in the diary data set. For merging, we’ll need to exclude those participants in totals_long who don’t have a match in diary. But first, I check how much the sum of total time across apps per day in total_long converges with the sum of time across all apps per day in apps_long. There will definitely be differences, simply because participants have more chances to make typos when reporting the individual apps per day (in apps_long) compared to reporting the overall time across all apps once per day (in total_long). Also, participants will occasionally skip an individual app per day rating. Below, I get the total time per day across all ten apps and then join that estimate with the data set containing the total times (total_long). Afterwards, per day, I see how much the two variables differ. # get total time across apps per day apps_aggregated &lt;- apps_long %&gt;% group_by(id, time_frame) %&gt;% filter(time_frame != &quot;week&quot;) %&gt;% # no week summarise(total_time = sum(social_media_objective, na.rm = TRUE)) # join with total times compare &lt;- left_join( totals_long %&gt;% filter(time_frame != &quot;week&quot;), apps_aggregated ) %&gt;% # and compute a deviance score mutate( difference = social_media_objective - total_time ) Overall, the two sums of use across apps and days are highly similar, with a mean difference of M = 4.09, SD = 17.4 and an extremely high correlation, r = 0.98. There were some missing values in the total social media time per day that participants reported. I check who has missing data on any of the weekdays in the totals_long data set. We can check whether those participants have reported individual app use for that day and use an aggregate over apps on that day as a substitute, given their high correlation. In other words: When participants forgot to report their overall social media use for a day, but reported use for all social media apps they used on that day, we can just use the sum of those social media apps as a replacement for overall social media use. However, we see in 2.10 that all participants reported social media screen time across all days, so no need for imputation. Table 2.10: Amount of missing values in data comparing total screen time id time_frame hours minutes social_media_objective total_time difference 0 0 0 0 0 0 0 Before I merge all data sets, I need to add the objective notifications and pickups (currently in apps_long) to the totals_long data set. Note that at the time of the study, iOS screen time wouldn’t report notifications for app per day, only for the past week. Therefore, the weekly notifications have the same caveat as the weekly screen time: they will include a couple of notifications that participants received on the day they retrospectively reported their objective use. For pickups, as with screen time, we can sum up all days to get an accurate measure of week’s pickups. I’ll sum up both pickups and notifications over participant. Pickups will be per day, but notifications only per week. I then add pickups to the totals_long data set. Notifications I’ll add later during merging (because they are not aggregated per day). # get total pickups across apps per day pickups_aggregated &lt;- apps_long %&gt;% group_by(id, time_frame) %&gt;% filter(time_frame != &quot;week&quot;) %&gt;% # no week summarise(pickups_objective = sum(pickups, na.rm = TRUE)) # get notifications across apps per participant notifications_aggregated &lt;- apps_long %&gt;% group_by(id) %&gt;% summarise(weekly_notifications = sum(notifications, na.rm = TRUE)) # add pickups to totals_long totals_long &lt;- left_join( totals_long, pickups_aggregated, by = c(&quot;id&quot;, &quot;time_frame&quot;) ) 2.4 Merge data sets Finally, we can merge the three data sets. The diary data set is the “master” because it has our variables of interest and the correct time structure. To that data set we add the objective social media measures from the totals_long data set. The identifier for a survey are the participant id and the day on which they filled out a survey/for which they reported objective use. Note that we omit the meta-data from totals_long (e.g., start_date) because they are different from the meta-data of diary. We also only keep the variables of interest from here on, meaning I’ll exclude some variables that aren’t informative. study1 &lt;- left_join( # the diary data with only relevant variables diary %&gt;% select( duration_diary = duration, # so that we can get filling out times for each data set id:day, pickups_subjective:enjoyable, # don&#39;t need individual minutes and hours anymore social_media_subjective:relatedness_state ), # the objective totals with only relevant variables totals_long %&gt;% filter(time_frame != &quot;week&quot;) %&gt;% rename( day = time_frame ) %&gt;% select(id, day, social_media_objective, pickups_objective), # joining by the two identifiers by = c(&quot;id&quot;, &quot;day&quot;) ) The merging above kept all cases of diary and added variables from totals_long when there was a match, otherwise setting the new variables to NA. Therefore, I remove all cases which have NA on the added variables to get to our final sample. (There were no missings in the social_media_objective variable.) study1 &lt;- study1 %&gt;% filter(!is.na(social_media_objective)) For descriptive information, I’ll also add the survey duration of the screen time data. study1 &lt;- left_join( study1, screen_time %&gt;% select(id, duration) %&gt;% rename( duration_screen_time = duration ), by = &quot;id&quot; ) Okay, then I add the personality variables. Again, study1 is the “master” and we add personality variables (aka traits from personality), omitting variables that aren’t of interest. study1 &lt;- left_join( study1, personality %&gt;% select( duration_personality = duration, id, bpns_1:openness ), by = &quot;id&quot; ) Then, I add the notifications over one week as a constant. study1 &lt;- left_join( study1, notifications_aggregated, by = &quot;id&quot; ) Last, some housekeeping, reordering variables and making sure they all have the correct variable type, plus reordering the day factor. study1 &lt;- study1 %&gt;% select( id, age:ethnicity, gender, starts_with(&quot;duration&quot;), day, social_media_subjective, pickups_subjective, notifications_subjective, social_media_objective, pickups_objective, weekly_notifications, well_being_state:relatedness_state, low_positive_peaceful:enjoyable, autonomy_trait:openness, bpns_1:big_five_45 ) %&gt;% mutate( id = droplevels(id), day = as.factor(day), day = fct_relevel( day, &quot;monday&quot;, &quot;tuesday&quot;, &quot;wednesday&quot;, &quot;thursday&quot;, &quot;friday&quot; ) ) When plotting the app data I want them to reflect the final sample, which is why I only keep those participants and surveys in the apps_long data set that are also in the final data set (i.e., study1). I can’t just exclude those participants in apps_long because I also excluded surveys in diary on which study1 is based. Therefore, I only keep those entries in apps_long which have a corresponding row in study1 (aka entries in both id and day). Also, I’ll remove “week” as a time frame and add the notifications for that app over whole week as a constant per app. Sidenote: The participants with a failed attention check in the personality data are not included in study1 because they were excluded at other steps during merging, so no need to manually remove them here. apps_week &lt;- apps_long %&gt;% select( id, time_frame, app, rank, notifications ) %&gt;% filter(time_frame == &quot;week&quot;) %&gt;% select(-time_frame) apps_long &lt;- left_join( # only those participants and days that are in final data set study1 %&gt;% select(id, day), # and the rest of the variables from apps_long, but without week apps_long %&gt;% rename(day = time_frame) %&gt;% filter(day != &quot;week&quot;) %&gt;% select(-notifications), by = c(&quot;id&quot;, &quot;day&quot;) ) %&gt;% # add the notifications for that app per week as a constant left_join( ., apps_week, by = c(&quot;id&quot;, &quot;app&quot;, &quot;rank&quot;) ) %&gt;% select(id, day, rank:notifications) %&gt;% rename(notifications_per_week = notifications) "],
["descriptives-and-visualizations-study-1.html", "3 Descriptives and Visualizations Study 1 3.1 Overview 3.2 Meta-level 3.3 Person-level 3.4 App-level 3.5 Day level", " 3 Descriptives and Visualizations Study 1 3.1 Overview In this section, I describe and visualize the sample and variables. We have variables on the meta-level (about the survey), the person-level, the app-level, and the day-level. App-level data is in the apps_long data file; all other in the study1 data file. Meta-level Duration of the entry survey, when participants reported traits (duration_personality) Duration of the exit survey, when participants reported their screen time (duration_screen_time) Person-level Participant identifier (id) age in years ethnicity Notifications of social media apps over the past week (weekly_notifications) Basic Psychological Need Satisfaction (autonomy_trait, competence_trait, relatedness_trait) plus their individual items (starting with bpns_) Big Five (extraversion, agreeableness, conscientiousness, neuroticsim, openness) plus their individual items (starting with big_five_) App-level What app participants report use for (app) On what rank was that app on participants’ top ten (rank) Notifications for that app for the week (notifications_per_week) Pickups for that app on that day (pickups) Screen time for that app on that day (social_media_objective) Day-level Duration of the survey on that day (duration_diary) day the survey was answered Estimated time on social media on that day (social_media_subjective) Estimated pickups of social media apps on that day (pickups_subjective) Estimated notifications of social media apps on that day (notifications_subjective) Objective time on social media on that day (social_media_objective) Objective pickups of social media apps on that day (pickups_objective) Well-being on that day (well_being) plus its individual items (starting with low_ and high_) Basic psychological needs on that day (autonomy_state, competence_state, relatedness_state) plus their individual items (starting with autonomy_, competence_, relatedness_ respectively) Experiences of satisfaction, boredom, stress, enjoyment on that day (satisfied, boring, stressful, enoyable) 3.2 Meta-level I begin with describing and plotting the duration of the entry and exit surveys. Table 3.1 shows descriptive stats; Figure 3.1 shows that twp participants had their entry surveys open for a day before pressing send, which skews the mean massively. However, those people’s data look good, so I wouldn’t exclude them here. Note: Colors are from here. Table 3.1: Duration of entry and exit surveys variable mean sd median min max range cilow cihigh duration_personality 1H 6M 31S 4H 38M 19S 15M 54S 1M 22S 1d 9H 33M 36S 1d 9H 32M 14S 9M 49S 2H 3M 13S duration_screen_time 20M 25S 20M 14S 13M 14S 1M 8S 1H 51M 29S 1H 50M 21S 16M 18S 24M 33S Figure 3.1: Duration of surveys 3.3 Person-level Let’s have a look at the final sample. Overall, our sample size is N = 95. The sample has a mean age of M = 20, SD = 1. The sample consists mostly of women (66 women, 29 men, and one non-binary participant). Most participants are Asian, followed by White, Black, and Hispanic, see Table 3.2 Table 3.2: Ethnicity of the sample ethnicity count percent Asian 40 42 White 26 27 Black or African American 11 12 Hispanic or Latino 10 11 Multiracial 6 6 Native Hawaiian or Other Pacific Islander 1 1 NA 1 1 Alright, next we look at the objective count of notifications over the past week, aggregated across all apps. Table 3.3 shows that participants received quite a lot of notifications from social media apps only. That distribution is heavily skewed (Figure 3.2 by a couple of participants who received several thousand notifications over the week. Table 3.3: Weekly notifications (objective) across all apps variable mean sd median min max range cilow cihigh weekly_notifications 1006.789 670.9589 870 126 3382 3256 870.1082 1143.471 Figure 3.2: Weekly notifications (objective) across all apps Now we look at the trait variables: the basic psychological need satisfaction and the big five. Note that I follow recent recommendations and calculate \\(\\omega\\) for reliability. Table 3.4 shows the descriptive information of the three psychological needs and the big five. Figure 3.3 shows their distribution. The sample isn’t too large, so considering the small size, I’d say everything looks pretty good. Table 3.4: Descriptives for trait variables variable mean sd median min max range cilow cihigh omega autonomy_trait 4.51 0.88 4.50 1.25 6.88 5.62 4.33 4.69 0.82 competence_trait 5.16 0.71 5.38 3.50 6.50 3.00 5.02 5.31 0.76 relatedness_trait 4.53 1.03 4.50 2.38 6.62 4.25 4.32 4.74 0.88 extraversion 3.13 0.75 3.00 1.62 5.00 3.38 2.98 3.29 0.86 agreeableness 3.69 0.55 3.62 2.33 5.00 2.67 3.58 3.81 0.75 conscientiousness 3.51 0.58 3.56 2.00 4.89 2.89 3.39 3.63 0.75 neuroticism 3.29 0.50 3.38 2.00 4.38 2.38 3.19 3.40 0.59 openness 3.54 0.48 3.60 2.10 4.80 2.70 3.44 3.64 0.67 Figure 3.3: Distribution of trait variables In Figure 3.4 we see the correlations between those traits. As expected psychological needs are correlated highly with each other. Credit for the lm lines goes to data prone, whose idea I adapted. Figure 3.4: Correlation matrix of trait level variables 3.4 App-level First, Figure 3.5 shows what apps mostly nominated (i.e., used). We see that out of the sample, most participants had Messaging, Snapchat, Whatsapp etc. as part of their top ten. Figure 3.5: Percentage of nominated apps Next, I visualize how many minutes each of those apps was used across the sample. For that, I need to reshape the data a bit to get the mean minutes per app across all days and all participants. In Figure 3.6, I show the average objective time per app. Note that the CIs are across the entire data and not nested by app or day. Also, a high mean doesn’t mean that much because it could just be from one participant who used it a lot on two days. The size of the points shows how often an app was reported across the entire sample. For apps that only had one or two entries, those CI will be nonexistent/small. In addition, I now exclude entries on social_media_objective that have NA. The NA here can mean participants just didn’t fill in anything, or they had zero duration on that day. Because adding up the raw scores across apps was so close to the daily total, I’ll exclude NAs here. Figure 3.6: Average daily objective time for all apps across participants and days I’ll do the same for objective pickups per app, averaged across day and participant. Figure 3.7 shows that the same apps that got a lot of screen time had a lot of pickups. Figure 3.7: Average daily pickups for all apps across participants and days Last, I check which apps got the most notifications over the week in Figure 3.8, on average. It’s interesting to see that Facebook had a lot of screen time and pickups, but much fewer notifications. Also, these notifications are per week, and not per day, as the previous two figures. Figure 3.8: Average notifications per week for all apps across participants 3.5 Day level Alright, we’re at the most interesting section, the daily surveys. I first look at how long people typically took for a survey. Table 3.5 shows that the mean is highly skewed because of outliers and the median more appropriate to describe the duration. In Figure 3.9 we see that a couple of people took a long time from opening to submitting the survey. I checked those participants who took a long time in the data processing section. Table 3.5: Duration of daily surveys variable mean sd median min max range cilow cihigh duration_diary 45M 10S 1H 34M 5S 16M 18S 3M 13S 17H 6M 27S 17H 3M 14S 36M 16S 54M 4S Figure 3.9: Duration of daily surveys Alright, next I inspect overall response rate in the final sample, aka how many valid surveys do we have among the final sample. Each participant received five surveys, one for each day, so 95 participants x 5 = 475. We have 431 surveys in the final sample, which means a 91% response rate among the final sample. Let’s inspect response rate per day. As is to be expected, participants lost motivation over the course of the week. However, even the response rate on Friday is really high (at least among our sample of valid responses). We should still consider to take the day grouping into account when modelling the data later in the analysis. Figure 3.10: Survey responses per day Next, I describe and plot the distributions of the social media use variables. The distribution and CI is of the entire sample, not aggregated by participant or day first. Table 3.6 shows that participants weren’t too far off in their estimates, which is interesting. As expected (Figure 3.11), the social media variables are a bit skewed, but overall, they look fine. Table 3.6: Descriptive information on social media variables variable mean sd median min max range cilow cihigh social_media_subjective 148 113 120 0 692 692 137 159 social_media_objective 137 94 114 0 565 565 128 146 pickups_subjective 34 43 16 0 259 259 30 38 pickups_objective 45 31 41 0 163 163 42 48 notifications_subjective 62 100 30 0 700 700 53 72 Figure 3.11: Distribution of social media variables I also want to see how much variability there is between the objective and subjective measures. In Figure 3.12 we see per participant the difference between objective and subjective social media use. The numbers in the grey box show whether the subjective report is an underestimate (negative number) or overestimate (positive number). Inspiration for the plot from here and here. Figure 3.12: Difference between subjective and objective social media use (difference in grey box) Now let’s look at the state well-being and psychological needs variables plus the four experiences (e.g., boring). Again, I calculate \\(\\omega\\), but this time for the entire sample in Table 3.7. That will necessarily bias the estimate because there’s multiple measures per person. I’m not aware of a consensus reliability procedure for repeated measures. Figure 3.13 shows that the data look pretty good. Table 3.7: Descriptives for state variables variable mean sd median min max range cilow cihigh omega well_being_state 3.24 0.71 3.25 1.25 5 3.75 3.17 3.30 0.85 autonomy_state 4.50 1.17 4.25 1.00 7 6.00 4.39 4.61 0.69 competence_state 4.62 1.27 4.50 1.00 7 6.00 4.50 4.74 0.80 relatedness_state 5.35 1.07 5.50 2.50 7 4.50 5.25 5.45 0.70 satisfied 4.62 1.34 5.00 1.00 7 6.00 4.50 4.75 NA boring 3.39 1.56 3.00 1.00 7 6.00 3.24 3.54 NA stressful 3.96 1.82 4.00 1.00 7 6.00 3.79 4.13 NA enjoyable 4.28 1.42 4.00 1.00 7 6.00 4.14 4.41 NA Figure 3.13: Distribution of state variables In Figure 3.14 we see the correlations between variables on the state level. Figure 3.14: Correlation matrix of state level variables. social = screen time on social media; _s = subjective; _o = objective; not = notifications In Figure 3.15 we see the correlations between use variables on the state level and the trait level. Figure 3.15: Correlation matrix of use variables (state) and personality traits. social = screen time on social media; _s = subjective; _o = objective; not = notifications; extra = extraversion; agree = agreeableness; con = conscientiousness; neuo = neuroticism; open = openness "]
]
